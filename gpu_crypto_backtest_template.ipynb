{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jadenfix/mft_crypto_research/blob/main/gpu_crypto_backtest_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab4dcc9f",
      "metadata": {
        "id": "ab4dcc9f"
      },
      "source": [
        "# GPUâ€‘Optimized Crypto ML Backtesting\n",
        "\n",
        "This notebook ingests 1â€‘minute OHLCV data for **ADA, BTC, ETH, and SOL**, trains **GPUâ€‘accelerated** models, and backtests a rolling metaâ€‘strategy that **reâ€‘trains every six hours (â‰ˆâ€¯360 bars) and adjusts ensemble weights every 60 executed trades**.\n",
        "\n",
        "> **Prerequisites**\n",
        "> * NVIDIA T4 (or compatible) GPU with CUDA drivers\n",
        "> * Python â‰¥3.10\n",
        "> * The libraries installed in the next cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08490143",
      "metadata": {
        "id": "08490143"
      },
      "outputs": [],
      "source": [
        "# !pip -q install polars numpy pandas scikit-learn xgboost lightgbm catboost backtesting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32659bfe",
      "metadata": {
        "id": "32659bfe"
      },
      "outputs": [],
      "source": [
        "import polars as pl, numpy as np, pandas as pd\n",
        "from collections import deque\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostRegressor\n",
        "from backtesting import Backtest, Strategy\n",
        "\n",
        "# Reproducibility\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "402f4a0f",
      "metadata": {
        "id": "402f4a0f"
      },
      "source": [
        "## Configuration â€“ paths and constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e56dd2c2",
      "metadata": {
        "id": "e56dd2c2"
      },
      "outputs": [],
      "source": [
        "# ðŸ‘‰ Edit these paths to point to your CSV files\n",
        "TRAIN_PATHS = {\n",
        "    'ada': 'ml_training_ada.csv',\n",
        "    'btc': 'ml_training_btc.csv',\n",
        "    'eth': 'ml_training_eth.csv',\n",
        "    'sol': 'ml_training_sol.csv',\n",
        "}\n",
        "\n",
        "BACKTEST_PATHS = {\n",
        "    'ada': 'backtest_ada.csv',\n",
        "    'btc': 'backtest_btc.csv',\n",
        "    'eth': 'backtest_eth.csv',\n",
        "    'sol': 'backtest_sol.csv',\n",
        "}\n",
        "\n",
        "RETRAIN_EVERY   = 360   # bars (~6 hours)\n",
        "REWEIGHT_TRADES = 60    # trades between dynamic weight refreshes\n",
        "WINDOW_LEN      = 10_000  # rolling buffer length\n",
        "TARGET_SUFFIX   = '_r_next'  # Adjust if your files use a different target name"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c4958b5",
      "metadata": {
        "id": "0c4958b5"
      },
      "source": [
        "## Helper functions â€“ verification & interpolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fca0d374",
      "metadata": {
        "id": "fca0d374"
      },
      "outputs": [],
      "source": [
        "def verify_cols(df: pl.DataFrame, asset: str) -> pl.DataFrame:\n",
        "    \"\"\"Ensure OHLCV columns are present (e.g., 'btc_open', 'btc_close', ...).\"\"\"\n",
        "    needed = [f'{asset}_{c}' for c in ('open', 'high', 'low', 'close', 'volume')]\n",
        "    missing = [c for c in needed if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"{asset}: missing columns {missing}\")\n",
        "    return df\n",
        "\n",
        "def read_csv_interp(path: str, asset: str) -> pl.DataFrame:\n",
        "    \"\"\"Read CSV â†’ verify columns â†’ sort by timestamp â†’ linearâ€‘interpolate numeric cols.\"\"\"\n",
        "    df = pl.read_csv(path, try_parse_dates=True, infer_schema_length=0)\n",
        "    df = verify_cols(df, asset)\n",
        "    if 'timestamp' in df.columns:\n",
        "        df = df.sort('timestamp')\n",
        "    num_cols = [c for c, dt in zip(df.columns, df.dtypes)\n",
        "                if pl.datatypes.is_numeric_dtype(dt)]\n",
        "    df = df.with_columns(pl.col(num_cols).interpolate().forward().backward())\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7c73639",
      "metadata": {
        "id": "a7c73639"
      },
      "source": [
        "## GPUâ€‘ready model factory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebc72cab",
      "metadata": {
        "id": "ebc72cab"
      },
      "outputs": [],
      "source": [
        "XGB_GPU_KW = dict(tree_method='gpu_hist', predictor='gpu_predictor', gpu_id=0, max_bin=256)\n",
        "LGB_GPU_KW = dict(device='gpu', gpu_platform_id=0, gpu_device_id=0)\n",
        "CAT_GPU_KW = dict(task_type='GPU', devices='0')\n",
        "\n",
        "def fit_models_on_gpu(X: np.ndarray, y: np.ndarray):\n",
        "    models = {\n",
        "        'xgb': XGBRegressor(**XGB_GPU_KW, n_estimators=400, learning_rate=0.05,\n",
        "                           max_depth=6, random_state=42),\n",
        "        'lgb': lgb.LGBMRegressor(**LGB_GPU_KW, n_estimators=400, learning_rate=0.05,\n",
        "                               max_depth=-1, random_state=42),\n",
        "        'cat': CatBoostRegressor(**CAT_GPU_KW, iterations=400, learning_rate=0.05,\n",
        "                                 depth=6, verbose=False, random_state=42),\n",
        "        'rf' : RandomForestRegressor(n_estimators=300, max_depth=None,\n",
        "                                     n_jobs=-1, random_state=42),  # CPU fallback\n",
        "    }\n",
        "    return {k: m.fit(X, y) for k, m in models.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcdbcd16",
      "metadata": {
        "id": "dcdbcd16"
      },
      "source": [
        "## Metaâ€‘strategy definition â€“ dynamic retraining & weighting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1c0bb59",
      "metadata": {
        "id": "a1c0bb59"
      },
      "outputs": [],
      "source": [
        "def build_meta_strategy(asset: str, base_models: dict, feat_cols: list[str],\n",
        "                        trade_threshold: float = 0.0,\n",
        "                        retrain_every: int = RETRAIN_EVERY,\n",
        "                        reweight_every_trades: int = REWEIGHT_TRADES,\n",
        "                        window_len: int = WINDOW_LEN):\n",
        "    class MetaVoteStrategyV2(Strategy):\n",
        "        def init(self):\n",
        "            self.models    = base_models\n",
        "            self.weights   = {k: 1/len(base_models) for k in base_models}\n",
        "            self.retrain_c = 0\n",
        "            self.trade_c   = 0\n",
        "            self.X_buf     = deque(maxlen=window_len)\n",
        "            self.y_buf     = deque(maxlen=window_len)\n",
        "\n",
        "        def _pred(self):\n",
        "            feats = np.array([self.data.df[col].iloc[-1] for col in feat_cols], dtype=np.float32).reshape(1, -1)\n",
        "            preds = {k: m.predict(feats)[0] for k, m in self.models.items()}\n",
        "            return sum(preds[k] * self.weights[k] for k in preds)\n",
        "\n",
        "        def _update_weights(self):\n",
        "            rets = self.data.df['Close'].pct_change().iloc[-60:]\n",
        "            if rets.isna().all():\n",
        "                return\n",
        "            mae = {}\n",
        "            X_recent = np.array(self.X_buf, dtype=np.float32)[-60:]\n",
        "            if len(X_recent) < 60:\n",
        "                return\n",
        "            for k, m in self.models.items():\n",
        "                y_hat = m.predict(X_recent)\n",
        "                mae[k] = np.mean(np.abs(y_hat - rets.values))\n",
        "            total = sum(1/(mae[k] + 1e-8) for k in mae)\n",
        "            self.weights = {k: (1/(mae[k] + 1e-8)) / total for k in mae}\n",
        "\n",
        "        def next(self):\n",
        "            # Buffer the previous bar's features/label\n",
        "            if len(self.data.df) > 1:\n",
        "                feat_vec = [self.data.df[col].iloc[-2] for col in feat_cols]\n",
        "                label = self.data.df['Close'].pct_change().iloc[-1]\n",
        "                if not np.isnan(label):\n",
        "                    self.X_buf.append(feat_vec)\n",
        "                    self.y_buf.append(label)\n",
        "\n",
        "            # Trading decision\n",
        "            z = self._pred()\n",
        "            if z > trade_threshold and not self.position.is_long:\n",
        "                self.position.close(); self.buy(); self.trade_c += 1\n",
        "            elif z < -trade_threshold and not self.position.is_short:\n",
        "                self.position.close(); self.sell(); self.trade_c += 1\n",
        "\n",
        "            # Scheduled retrain\n",
        "            self.retrain_c += 1\n",
        "            if self.retrain_c % retrain_every == 0 and len(self.X_buf) > 500:\n",
        "                X_new = np.asarray(self.X_buf, dtype=np.float32)\n",
        "                y_new = np.asarray(self.y_buf, dtype=np.float32)\n",
        "                self.models = fit_models_on_gpu(X_new, y_new)\n",
        "\n",
        "            # Scheduled weight refresh\n",
        "            if self.trade_c >= reweight_every_trades:\n",
        "                self._update_weights()\n",
        "                self.trade_c = 0\n",
        "\n",
        "    return MetaVoteStrategyV2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e69d3df",
      "metadata": {
        "id": "1e69d3df"
      },
      "source": [
        "## Load & prepare training data â€“ interpolation + simple features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc2532ac",
      "metadata": {
        "id": "bc2532ac"
      },
      "outputs": [],
      "source": [
        "data_train = {}\n",
        "feature_cols = {}\n",
        "\n",
        "for asset, path in TRAIN_PATHS.items():\n",
        "    df = read_csv_interp(path, asset)\n",
        "\n",
        "    # Create target column if not already present\n",
        "    if f'{asset}{TARGET_SUFFIX}' not in df.columns:\n",
        "        df = df.with_columns(\n",
        "            (pl.col(f'{asset}_close').pct_change().shift(-1)).alias(f'{asset}{TARGET_SUFFIX}')\n",
        "        )\n",
        "\n",
        "    # Simple features: percentage change of OHLCV\n",
        "    feats = []\n",
        "    for col in ('open', 'high', 'low', 'close', 'volume'):\n",
        "        base = f'{asset}_{col}'\n",
        "        df = df.with_columns(pl.col(base).pct_change().alias(f'{base}_ret'))\n",
        "        feats.append(f'{base}_ret')\n",
        "\n",
        "    data_train[asset] = df.drop_nulls()\n",
        "    feature_cols[asset] = feats\n",
        "\n",
        "print({k: data_train[k].shape for k in data_train})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a02ae067",
      "metadata": {
        "id": "a02ae067"
      },
      "source": [
        "## Train initial models (GPUâ€‘accelerated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17b26697",
      "metadata": {
        "id": "17b26697"
      },
      "outputs": [],
      "source": [
        "asset_models = {}\n",
        "for asset, df in data_train.items():\n",
        "    y = df[f'{asset}{TARGET_SUFFIX}'].to_numpy(dtype=np.float32)\n",
        "    X = df.select(feature_cols[asset]).to_numpy(dtype=np.float32)\n",
        "    print(f'Training models for {asset.upper()}:  X={X.shape}  y={y.shape}')\n",
        "    asset_models[asset] = fit_models_on_gpu(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d76432e4",
      "metadata": {
        "id": "d76432e4"
      },
      "source": [
        "## Example backtest â€“ choose an asset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9069d54",
      "metadata": {
        "id": "f9069d54"
      },
      "outputs": [],
      "source": [
        "asset = 'btc'  # Change as desired ('ada', 'eth', 'sol')\n",
        "bt_df = read_csv_interp(BACKTEST_PATHS[asset], asset)\n",
        "\n",
        "# The strategy expects 'Close' for price â€“ map the asset's close price\n",
        "bt_df = bt_df.rename({f'{asset}_close': 'Close'})\n",
        "\n",
        "# Backtesting.py needs a pandas DataFrame with datetime index\n",
        "bt_pd = bt_df.select(['timestamp', 'Close'] + feature_cols[asset]).to_pandas()\n",
        "bt_pd.set_index('timestamp', inplace=True)\n",
        "\n",
        "Strat = build_meta_strategy(asset, asset_models[asset], feature_cols[asset])\n",
        "\n",
        "bt = Backtest(bt_pd, Strat, cash=1_000_000, commission=0.0015, exclusive_orders=False)\n",
        "results = bt.run()\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e64b9457",
      "metadata": {
        "id": "e64b9457"
      },
      "source": [
        "## Visualize equity curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "093fe6b0",
      "metadata": {
        "id": "093fe6b0"
      },
      "outputs": [],
      "source": [
        "bt.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "662d6bcc",
      "metadata": {
        "id": "662d6bcc"
      },
      "source": [
        "### Next steps\n",
        "* Loop the backtest for all assets.\n",
        "* Enrich feature engineering (technical indicators, regime filters, etc.).\n",
        "* Experiment with hyperâ€‘parameters and different ensemble combinations.\n",
        "\n",
        "Happy tradingÂ ðŸš€"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}