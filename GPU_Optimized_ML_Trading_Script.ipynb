{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jadenfix/mft_crypto_research/blob/main/GPU_Optimized_ML_Trading_Script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jDlqh5kuwZQ6",
        "outputId": "a3f5b2f7-d95b-4e24-acfc-f513f9b87bac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: polars in /usr/local/lib/python3.11/dist-packages (1.21.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (18.1.0)\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Collecting ta\n",
            "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting Backtesting\n",
            "  Downloading backtesting-0.6.4-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: bokeh!=3.0.*,!=3.2.*,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from Backtesting) (3.7.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.11/dist-packages (from bokeh!=3.0.*,!=3.2.*,>=1.4.0->Backtesting) (3.1.6)\n",
            "Requirement already satisfied: narwhals>=1.13 in /usr/local/lib/python3.11/dist-packages (from bokeh!=3.0.*,!=3.2.*,>=1.4.0->Backtesting) (1.39.0)\n",
            "Requirement already satisfied: tornado>=6.2 in /usr/local/lib/python3.11/dist-packages (from bokeh!=3.0.*,!=3.2.*,>=1.4.0->Backtesting) (6.4.2)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.11/dist-packages (from bokeh!=3.0.*,!=3.2.*,>=1.4.0->Backtesting) (2025.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=2.9->bokeh!=3.0.*,!=3.2.*,>=1.4.0->Backtesting) (3.0.2)\n",
            "Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backtesting-0.6.4-py3-none-any.whl (191 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.4/191.4 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.1-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Building wheels for collected packages: ta\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=b398cc86b5ab9dc8fdff1e173a41634f0ca07a96b4af9a05ba8af7d1f1304fe2\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/d7/29/7781cc5eb9a3659d032d7d15bdd0f49d07d2b24fec29f44bc4\n",
            "Successfully built ta\n",
            "Installing collected packages: colorlog, alembic, ta, optuna, catboost, Backtesting\n",
            "Successfully installed Backtesting-0.6.4 alembic-1.16.1 catboost-1.2.8 colorlog-6.9.0 optuna-4.3.0 ta-0.11.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/backtesting/_plotting.py:55: UserWarning: Jupyter Notebook detected. Setting Bokeh output to notebook. This may not work in Jupyter clients without JavaScript support, such as old IDEs. Reset with `backtesting.set_bokeh_output(notebook=False)`.\n",
            "  warnings.warn('Jupyter Notebook detected. '\n",
            "<ipython-input-1-bd82f07de339>:87: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
            "  datetime_series = pd.to_datetime(pd.date_range(start='2024-01-01', periods=num_rows, freq='T'))\n",
            "<ipython-input-1-bd82f07de339>:87: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
            "  datetime_series = pd.to_datetime(pd.date_range(start='2024-01-01', periods=num_rows, freq='T'))\n",
            "<ipython-input-1-bd82f07de339>:87: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
            "  datetime_series = pd.to_datetime(pd.date_range(start='2024-01-01', periods=num_rows, freq='T'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO: Dummy files, if created, will be stored in: /tmp/pydummy_ml_data_iw_5d0ho\n",
            "\n",
            "--- Checking for training data files and creating dummies if necessary ---\n",
            "INFO: File found at processed_data_ada_long.csv. Using existing file for asset 'ada'.\n",
            "INFO: File found at processed_data_btc.csv. Using existing file for asset 'btc'.\n",
            "INFO: File found at processed_data_eth_long.csv. Using existing file for asset 'eth'.\n",
            "INFO: File found at processed_data_solana_long.csv. Using existing file for asset 'sol'.\n",
            "\n",
            "--- Checking for backtest data files and creating dummies if necessary ---\n",
            "INFO: File found at 2024_to_april_2025_ada_data.csv. Using existing file for asset 'ada'.\n",
            "WARNING: File 2024_to_april_2025_btc_data.csv for asset 'btc' not found. Creating a dummy CSV at: /tmp/pydummy_ml_data_iw_5d0ho/2024_to_april_2025_btc_data.csv\n",
            "INFO: Successfully created dummy file for asset 'btc': /tmp/pydummy_ml_data_iw_5d0ho/2024_to_april_2025_btc_data.csv\n",
            "WARNING: File 2024_to_april_2025_eth_data.csv for asset 'eth' not found. Creating a dummy CSV at: /tmp/pydummy_ml_data_iw_5d0ho/2024_to_april_2025_eth_data.csv\n",
            "INFO: Successfully created dummy file for asset 'eth': /tmp/pydummy_ml_data_iw_5d0ho/2024_to_april_2025_eth_data.csv\n",
            "WARNING: File 2024_to_april_2025_solana_data.csv for asset 'sol' not found. Creating a dummy CSV at: /tmp/pydummy_ml_data_iw_5d0ho/2024_to_april_2025_solana_data.csv\n",
            "INFO: Successfully created dummy file for asset 'sol': /tmp/pydummy_ml_data_iw_5d0ho/2024_to_april_2025_solana_data.csv\n",
            "\n",
            "--- Loading data ---\n",
            "Loading training data for ada from: processed_data_ada_long.csv\n",
            "Loading training data for btc from: processed_data_btc.csv\n",
            "Loading training data for eth from: processed_data_eth_long.csv\n"
          ]
        },
        {
          "ename": "ComputeError",
          "evalue": "found more fields than defined in 'Schema'\n\nConsider setting 'truncate_ragged_lines=True'.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mComputeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bd82f07de339>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Loading data ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m \u001b[0mraw_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mload_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mACTUAL_TRAIN_PATHS\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0mraw_bt\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mload_backtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mACTUAL_BACKTEST_PATHS\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-bd82f07de339>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Loading data ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m \u001b[0mraw_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mload_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mACTUAL_TRAIN_PATHS\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0mraw_bt\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mload_backtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mACTUAL_BACKTEST_PATHS\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-bd82f07de339>\u001b[0m in \u001b[0;36mload_training\u001b[0;34m(asset)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading training data for {asset} from: {ACTUAL_TRAIN_PATHS[asset]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     return (\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mACTUAL_TRAIN_PATHS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0masset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfer_schema_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100_000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         .with_columns(\n\u001b[1;32m    145\u001b[0m             \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/polars/_utils/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mold_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             )\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__signature__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/polars/_utils/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mold_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             )\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__signature__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/polars/_utils/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mold_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             )\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__signature__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/polars/io/csv/functions.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(source, has_header, columns, new_columns, separator, comment_prefix, quote_char, skip_rows, skip_lines, schema, schema_overrides, null_values, missing_utf8_is_empty_string, ignore_errors, try_parse_dates, n_threads, infer_schema, infer_schema_length, batch_size, n_rows, encoding, low_memory, rechunk, use_pyarrow, storage_options, skip_rows_after_header, row_index_name, row_index_offset, sample_size, eol_char, raise_if_empty, truncate_ragged_lines, decimal_comma, glob)\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         ) as data:\n\u001b[0;32m--> 534\u001b[0;31m             df = _read_csv_impl(\n\u001b[0m\u001b[1;32m    535\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m                 \u001b[0mhas_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/polars/io/csv/functions.py\u001b[0m in \u001b[0;36m_read_csv_impl\u001b[0;34m(source, has_header, columns, separator, comment_prefix, quote_char, skip_rows, skip_lines, schema, schema_overrides, null_values, missing_utf8_is_empty_string, ignore_errors, try_parse_dates, n_threads, infer_schema_length, batch_size, n_rows, encoding, low_memory, rechunk, skip_rows_after_header, row_index_name, row_index_offset, sample_size, eol_char, raise_if_empty, truncate_ragged_lines, decimal_comma, glob)\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mprojection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_columns_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m     pydf = PyDataFrame.read_csv(\n\u001b[0m\u001b[1;32m    683\u001b[0m         \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0minfer_schema_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mComputeError\u001b[0m: found more fields than defined in 'Schema'\n\nConsider setting 'truncate_ragged_lines=True'."
          ]
        }
      ],
      "source": [
        "!pip install polars pandas scikit-learn pyarrow catboost optuna lightgbm xgboost ta Backtesting matplotlib\n",
        "import polars as pl\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler # Not used in this version, but kept if needed later\n",
        "from sklearn.metrics import classification_report, mean_absolute_error # classification_report not used\n",
        "import ta\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "import lightgbm as lgb\n",
        "import optuna\n",
        "from backtesting import Backtest, Strategy\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import tempfile\n",
        "import traceback\n",
        "\n",
        "# --- Configuration Section ---\n",
        "# Define your intended TRAIN_PATHS and BACKTEST_PATHS.\n",
        "# If your files are located elsewhere, use absolute paths here.\n",
        "INTENDED_TRAIN_PATHS = {\n",
        "    \"ada\": \"processed_data_ada_long.csv\",\n",
        "    \"btc\": \"processed_data_btc.csv\",\n",
        "    \"eth\": \"processed_data_eth_long.csv\",\n",
        "    \"sol\": \"processed_data_solana_long.csv\",\n",
        "}\n",
        "\n",
        "INTENDED_BACKTEST_PATHS = {\n",
        "    \"ada\": \"2024_to_april_2025_ada_data.csv\",\n",
        "    \"btc\": \"2024_to_april_2025_btc_data.csv\",\n",
        "    \"eth\": \"2024_to_april_2025_eth_data.csv\",\n",
        "    \"sol\": \"2024_to_april_2025_solana_data.csv\",\n",
        "}\n",
        "\n",
        "TARGET_COL = {\n",
        "    \"ada\": \"target_next_close_ada\",\n",
        "    \"btc\": \"target_next_close_btc\",\n",
        "    \"eth\": \"target_next_close_eth\",\n",
        "    \"sol\": \"target_next_close_solana\",\n",
        "}\n",
        "\n",
        "PREFIX = {\n",
        "    \"ada\": \"ada\",\n",
        "    \"btc\": \"btc\",\n",
        "    \"eth\": \"eth\",\n",
        "    \"sol\": \"solana\",\n",
        "}\n",
        "\n",
        "# --- Global temporary directory for dummy files ---\n",
        "try:\n",
        "    _temp_dir_for_dummy_files = tempfile.mkdtemp(prefix=\"pydummy_ml_data_\")\n",
        "    print(f\"INFO: Dummy files, if created, will be stored in: {_temp_dir_for_dummy_files}\")\n",
        "except Exception as e:\n",
        "    print(f\"WARNING: Could not create a global temporary directory ({e}). Falling back to current directory for dummy files, which might fail if read-only.\")\n",
        "    _temp_dir_for_dummy_files = \".\"\n",
        "\n",
        "\n",
        "# --- Dummy file creation function (OSError Fix) ---\n",
        "def create_dummy_csv_if_not_exists(asset_label, original_path_from_config, is_training, base_temp_dir):\n",
        "    \"\"\"\n",
        "    Checks if a file exists at original_path_from_config.\n",
        "    If not, creates a dummy CSV in base_temp_dir and returns its path.\n",
        "    Otherwise, returns original_path_from_config.\n",
        "    \"\"\"\n",
        "    if os.path.exists(original_path_from_config):\n",
        "        print(f\"INFO: File found at {original_path_from_config}. Using existing file for asset '{asset_label}'.\")\n",
        "        return original_path_from_config\n",
        "\n",
        "    original_filename = os.path.basename(original_path_from_config)\n",
        "    dummy_file_path = os.path.join(base_temp_dir, original_filename)\n",
        "    print(f\"WARNING: File {original_path_from_config} for asset '{asset_label}' not found. Creating a dummy CSV at: {dummy_file_path}\")\n",
        "\n",
        "    num_rows = 500 # Increased rows for more stable TA in dummy data\n",
        "    if asset_label not in PREFIX:\n",
        "        raise ValueError(f\"Asset label '{asset_label}' not found in PREFIX. Cannot generate dummy columns.\")\n",
        "    if is_training and asset_label not in TARGET_COL:\n",
        "         raise ValueError(f\"Asset label '{asset_label}' not found in TARGET_COL. Cannot generate dummy target column.\")\n",
        "\n",
        "    asset_p = PREFIX[asset_label]\n",
        "    dummy_data = {}\n",
        "\n",
        "    if is_training:\n",
        "        dummy_data['timestamp'] = pd.to_datetime(pd.date_range(start='2022-01-01', periods=num_rows, freq='T')).strftime('%Y-%m-%dT%H:%M:%S.%f')\n",
        "    else:\n",
        "        datetime_series = pd.to_datetime(pd.date_range(start='2024-01-01', periods=num_rows, freq='T'))\n",
        "        dummy_data['date_only'] = datetime_series.strftime('%Y-%m-%d')\n",
        "        dummy_data['time_only'] = datetime_series.strftime('%H:%M:%S')\n",
        "\n",
        "    # More realistic OHLCV data\n",
        "    price_open = np.abs(np.random.normal(loc=100, scale=20, size=num_rows))\n",
        "    price_open[0] = 100 # Start at a defined point\n",
        "    price_open = np.maximum(price_open, 1) # Ensure positive prices\n",
        "\n",
        "    dummy_data[f'{asset_p}_open'] = price_open\n",
        "    dummy_data[f'{asset_p}_high'] = price_open + np.abs(np.random.normal(loc=2, scale=1, size=num_rows))\n",
        "    dummy_data[f'{asset_p}_low'] = price_open - np.abs(np.random.normal(loc=2, scale=1, size=num_rows))\n",
        "    dummy_data[f'{asset_p}_low'] = np.minimum(dummy_data[f'{asset_p}_low'], dummy_data[f'{asset_p}_open']) # Ensure low <= open\n",
        "    dummy_data[f'{asset_p}_low'] = np.maximum(dummy_data[f'{asset_p}_low'], 0.1) # Ensure positive low\n",
        "    dummy_data[f'{asset_p}_high'] = np.maximum(dummy_data[f'{asset_p}_high'], dummy_data[f'{asset_p}_open']) # Ensure high >= open\n",
        "\n",
        "    # Close price is within low and high\n",
        "    dummy_data[f'{asset_p}_close'] = np.random.uniform(dummy_data[f'{asset_p}_low'], dummy_data[f'{asset_p}_high'])\n",
        "    dummy_data[f'{asset_p}_volume'] = np.abs(np.random.normal(loc=1000, scale=500, size=num_rows)) + 100 # Ensure positive volume\n",
        "\n",
        "    if is_training:\n",
        "        target_col_name = TARGET_COL[asset_label]\n",
        "        dummy_data[target_col_name] = dummy_data[f'{asset_p}_close'] * (1 + np.random.normal(loc=0, scale=0.01, size=num_rows))\n",
        "\n",
        "    df = pd.DataFrame(dummy_data)\n",
        "    try:\n",
        "        os.makedirs(os.path.dirname(dummy_file_path), exist_ok=True)\n",
        "        df.to_csv(dummy_file_path, index=False)\n",
        "        print(f\"INFO: Successfully created dummy file for asset '{asset_label}': {dummy_file_path}\")\n",
        "        return dummy_file_path\n",
        "    except OSError as e:\n",
        "        print(f\"CRITICAL ERROR: Could not write dummy CSV for asset '{asset_label}' to {dummy_file_path}. OS error: {e}\")\n",
        "        raise\n",
        "\n",
        "# --- Paths to be used by the script (potentially updated to dummy file paths) ---\n",
        "ACTUAL_TRAIN_PATHS = INTENDED_TRAIN_PATHS.copy()\n",
        "ACTUAL_BACKTEST_PATHS = INTENDED_BACKTEST_PATHS.copy()\n",
        "\n",
        "print(\"\\n--- Checking for training data files and creating dummies if necessary ---\")\n",
        "for asset_key in list(ACTUAL_TRAIN_PATHS.keys()):\n",
        "    original_path = ACTUAL_TRAIN_PATHS[asset_key]\n",
        "    ACTUAL_TRAIN_PATHS[asset_key] = create_dummy_csv_if_not_exists(\n",
        "        asset_key, original_path, is_training=True, base_temp_dir=_temp_dir_for_dummy_files\n",
        "    )\n",
        "\n",
        "print(\"\\n--- Checking for backtest data files and creating dummies if necessary ---\")\n",
        "for asset_key in list(ACTUAL_BACKTEST_PATHS.keys()):\n",
        "    original_path = ACTUAL_BACKTEST_PATHS[asset_key]\n",
        "    ACTUAL_BACKTEST_PATHS[asset_key] = create_dummy_csv_if_not_exists(\n",
        "        asset_key, original_path, is_training=False, base_temp_dir=_temp_dir_for_dummy_files\n",
        "    )\n",
        "\n",
        "# --- Data Loading Functions ---\n",
        "def load_training(asset):\n",
        "    print(f\"Loading training data for {asset} from: {ACTUAL_TRAIN_PATHS[asset]}\")\n",
        "    return (\n",
        "        pl.read_csv(ACTUAL_TRAIN_PATHS[asset], infer_schema_length=100_000)\n",
        "        .with_columns(\n",
        "            pl.col(\"timestamp\")\n",
        "              .str.strptime(pl.Datetime, format=\"%Y-%m-%dT%H:%M:%S%.f\", strict=False)\n",
        "              .alias(\"ts\")\n",
        "        )\n",
        "        .sort(\"ts\")\n",
        "    )\n",
        "\n",
        "def load_backtest(asset):\n",
        "    print(f\"Loading backtest data for {asset} from: {ACTUAL_BACKTEST_PATHS[asset]}\")\n",
        "    return (\n",
        "        pl.read_csv(ACTUAL_BACKTEST_PATHS[asset], infer_schema_length=100_000)\n",
        "        .with_columns(\n",
        "            (\n",
        "                pl.concat_str([pl.col(\"date_only\"), pl.lit(\" \"), pl.col(\"time_only\")])\n",
        "                .str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\", strict=False)\n",
        "            ).alias(\"ts\")\n",
        "        )\n",
        "        .sort(\"ts\")\n",
        "    )\n",
        "\n",
        "print(\"\\n--- Loading data ---\")\n",
        "raw_train = {a: load_training(a)  for a in ACTUAL_TRAIN_PATHS}\n",
        "raw_bt    = {a: load_backtest(a)  for a in ACTUAL_BACKTEST_PATHS}\n",
        "\n",
        "\n",
        "# --- Feature Engineering ---\n",
        "def engineer_features(df: pl.DataFrame, asset: str, lags: int = 5) -> pl.DataFrame:\n",
        "    if df.is_empty():\n",
        "        print(f\"Warning: Input DataFrame for engineer_features for asset '{asset}' is empty. Returning empty DataFrame.\")\n",
        "        return pl.DataFrame()\n",
        "\n",
        "    pref = PREFIX[asset]\n",
        "    ohlcv_cols_mapping = {}\n",
        "    rename_map_for_ta = {} # This will map original column names to \"open\", \"high\", \"low\", \"close\", \"volume\"\n",
        "\n",
        "    for col_type in [\"open\", \"high\", \"low\", \"close\", \"volume\"]:\n",
        "        prefixed_name = f\"{pref}_{col_type}\"\n",
        "        if prefixed_name in df.columns:\n",
        "            rename_map_for_ta[prefixed_name] = col_type\n",
        "        elif col_type in df.columns: # Fallback to generic name\n",
        "            rename_map_for_ta[col_type] = col_type\n",
        "\n",
        "    if not all(ct in rename_map_for_ta.values() for ct in [\"high\", \"low\", \"close\"]):\n",
        "        raise ValueError(f\"Asset {asset}: Could not find required HLC columns for TA. Found mappings: {rename_map_for_ta}. Available df columns: {df.columns}\")\n",
        "\n",
        "    base = df.select(list(rename_map_for_ta.keys())).rename(rename_map_for_ta)\n",
        "    p = base.to_pandas()\n",
        "\n",
        "    for col_name in [\"open\", \"high\", \"low\", \"close\", \"volume\"]:\n",
        "        if col_name in p.columns:\n",
        "            p[col_name] = pd.to_numeric(p[col_name], errors='coerce')\n",
        "\n",
        "    p.fillna(method='ffill', inplace=True) # Fill NaNs before TA\n",
        "    p.fillna(method='bfill', inplace=True) # Fill remaining NaNs at the beginning\n",
        "\n",
        "    if p[\"close\"].isnull().all() or p[\"high\"].isnull().all() or p[\"low\"].isnull().all():\n",
        "        print(f\"Warning: Asset {asset} - HLC columns are all NaN after processing. TA features will be NaN.\")\n",
        "        ta_feature_names = [\"rsi14\", \"stochk\", \"roc10\", \"macd\", \"bb_w\", \"atr14\", \"obv\"]\n",
        "        for ta_f in ta_feature_names: p[ta_f] = np.nan\n",
        "    else:\n",
        "        p[\"rsi14\"]  = ta.momentum.RSIIndicator(p[\"close\"], 14, fillna=True).rsi()\n",
        "        p[\"stochk\"] = ta.momentum.StochasticOscillator(p[\"high\"], p[\"low\"], p[\"close\"], 14, fillna=True).stoch()\n",
        "        p[\"roc10\"]  = ta.momentum.ROCIndicator(p[\"close\"], 10, fillna=True).roc()\n",
        "        p[\"macd\"]   = ta.trend.MACD(p[\"close\"], fillna=True).macd()\n",
        "        p[\"bb_w\"]   = ta.volatility.BollingerBands(p[\"close\"], fillna=True).bollinger_wband()\n",
        "        p[\"atr14\"]  = ta.volatility.AverageTrueRange(p[\"high\"], p[\"low\"], p[\"close\"], 14, fillna=True).average_true_range()\n",
        "        if \"volume\" in p.columns and not p[\"volume\"].isnull().all() and (p[\"volume\"] != 0).any() :\n",
        "            p[\"obv\"] = ta.volume.OnBalanceVolumeIndicator(p[\"close\"], p[\"volume\"], fillna=True).on_balance_volume()\n",
        "        else:\n",
        "            p[\"obv\"] = 0.0\n",
        "\n",
        "    f = pl.from_pandas(p)\n",
        "    cols_to_drop_after_ta = list(rename_map_for_ta.values())\n",
        "\n",
        "    for w in (20, 60, 120):\n",
        "        f = f.with_columns([\n",
        "            pl.col(\"close\").rolling_mean(w, min_periods=max(1, w//2)).alias(f\"sma{w}\"),\n",
        "            pl.col(\"close\").rolling_std(w, min_periods=max(1, w//2)).alias(f\"vol{w}\"),\n",
        "            (pl.col(\"high\") - pl.col(\"low\")).rolling_mean(w, min_periods=max(1, w//2)).alias(f\"hl_range{w}\")\n",
        "        ])\n",
        "    for l_val in range(1, lags + 1):\n",
        "        f = f.with_columns([\n",
        "            pl.col(\"close\").shift(l_val).alias(f\"close_lag{l_val}\"),\n",
        "            pl.col(\"rsi14\").shift(l_val).alias(f\"rsi_lag{l_val}\")\n",
        "        ])\n",
        "    return f.drop(cols_to_drop_after_ta)\n",
        "\n",
        "# --- Verify counts ---\n",
        "print(\"\\n--- Verifying feature counts ---\")\n",
        "for asset_key in ACTUAL_TRAIN_PATHS:\n",
        "    if asset_key in raw_train and not raw_train[asset_key].is_empty():\n",
        "        try:\n",
        "            engineered_df = engineer_features(raw_train[asset_key], asset_key)\n",
        "            if not engineered_df.is_empty():\n",
        "                feat_cols = engineered_df.columns\n",
        "                print(f\"{asset_key}: {len(feat_cols)} engineered vars → {feat_cols[:8]}\")\n",
        "            else:\n",
        "                print(f\"{asset_key}: Feature engineering resulted in an empty DataFrame.\")\n",
        "        except Exception as e_eng:\n",
        "            print(f\"Error engineering features for {asset_key}: {e_eng}\")\n",
        "            traceback.print_exc()\n",
        "    else:\n",
        "        print(f\"{asset_key}: Raw training data is empty or not loaded for feature count verification.\")\n",
        "\n",
        "\n",
        "# --- Build X / y ---\n",
        "def build_matrix(asset: str):\n",
        "    if asset not in raw_train or raw_train[asset].is_empty():\n",
        "        print(f\"Warning: Asset {asset} - Raw training data is empty for build_matrix.\")\n",
        "        return np.array([]).reshape(0,0), np.array([])\n",
        "\n",
        "    feats = engineer_features(raw_train[asset], asset)\n",
        "\n",
        "    if feats.is_empty():\n",
        "        print(f\"Warning: Asset {asset} - Features DataFrame is empty before combining with target.\")\n",
        "        return np.array([]).reshape(0,0), np.array([])\n",
        "\n",
        "    target_column_name = TARGET_COL[asset]\n",
        "    if target_column_name not in raw_train[asset].columns:\n",
        "        raise ValueError(f\"Target column '{target_column_name}' not found in raw_train['{asset}']. Available: {raw_train[asset].columns}\")\n",
        "\n",
        "    num_feat_cols = feats.width\n",
        "\n",
        "    # Combine features with target and then drop NaNs\n",
        "    # Ensure target is aligned with features (Polars does this by row number if not joining by key)\n",
        "    # It's safer to ensure row counts match or handle alignment explicitly if needed.\n",
        "    # Assuming raw_train[asset] and feats have compatible row structures before this point.\n",
        "    # If engineer_features drops rows, this needs careful handling.\n",
        "    # For simplicity, assuming engineer_features preserves row order and count relative to what's needed for target.\n",
        "\n",
        "    # A robust way: convert target to series, add as column, then drop nulls.\n",
        "    # Polars handles Series addition by matching row numbers if DataFrame shapes are compatible.\n",
        "    target_series = raw_train[asset][target_column_name]\n",
        "\n",
        "    # If feature engineering changed row count (e.g. due to shifts creating initial NaNs that are not dropped yet)\n",
        "    # we need to align target_series. This is complex.\n",
        "    # Simpler: assume engineer_features outputs df with same row count as input for now, or handles it.\n",
        "    # The drop_nulls after with_columns is key.\n",
        "\n",
        "    mat = feats.with_columns(target_series.alias(target_column_name)).drop_nulls()\n",
        "\n",
        "    if mat.is_empty():\n",
        "        print(f\"Warning: Asset {asset} - DataFrame is empty after combining features, target, and drop_nulls().\")\n",
        "        return np.array([]).reshape(0, num_feat_cols), np.array([])\n",
        "\n",
        "    X = mat.drop(target_column_name).to_numpy()\n",
        "    y = mat[target_column_name].to_numpy()\n",
        "    return X, y\n",
        "\n",
        "print(\"\\n--- Building datasets (X, y matrices) ---\")\n",
        "datasets = {a: build_matrix(a) for a in ACTUAL_TRAIN_PATHS.keys()}\n",
        "\n",
        "for a_key,(X_val,y_val) in datasets.items():\n",
        "    print(f\"{a_key}: {X_val.shape[1] if X_val.ndim == 2 and X_val.shape[0] > 0 else 0} features  |  samples {X_val.shape[0]}\")\n",
        "\n",
        "lengths = {a: y.shape[0] for a, (X, y) in datasets.items() if X.ndim == 2 and X.shape[0] > 0 and y.ndim == 1 and y.shape[0] > 0}\n",
        "if not lengths:\n",
        "    print(\"CRITICAL: All datasets are empty or have zero length after build_matrix. Cannot proceed with trimming or training.\")\n",
        "    min_n = 0\n",
        "else:\n",
        "    min_n = min(lengths.values())\n",
        "\n",
        "print(\"Samples before trim (from valid datasets):\", lengths)\n",
        "print(f\"→ Trimming all to {min_n} rows (smallest non-empty dataset's length)\")\n",
        "\n",
        "datasets_even = {}\n",
        "for a_d, (X_d, y_d) in datasets.items():\n",
        "    if X_d.ndim == 2 and X_d.shape[0] > 0 and y_d.ndim == 1 and y_d.shape[0] > 0 and X_d.shape[0] >= min_n and min_n > 0 :\n",
        "        datasets_even[a_d] = (X_d[-min_n:], y_d[-min_n:])\n",
        "    else:\n",
        "        datasets_even[a_d] = (X_d, y_d) # Keep as is if conditions not met\n",
        "\n",
        "for a_de, (X_e, y_e) in datasets_even.items():\n",
        "    print(f\"{a_de}: {X_e.shape[0]} samples, {X_e.shape[1] if X_e.ndim==2 and X_e.shape[0]>0 else 0} features after trimming attempt\")\n",
        "\n",
        "datasets = datasets_even\n",
        "\n",
        "\n",
        "# ───────────────────────────────────────── 2. Model Tuning ─────────────────────────────────────────\n",
        "print(\"\\n--- Model Tuning Setup ---\")\n",
        "\n",
        "class PurgedKFold(KFold):\n",
        "    def __init__(self, n_splits=8, embargo=10):\n",
        "        super().__init__(n_splits=n_splits, shuffle=False)\n",
        "        self.embargo = embargo\n",
        "\n",
        "    def split(self, X, y=None, groups=None):\n",
        "        if not hasattr(X, 'shape') or not hasattr(X, '__len__'): X_arr = np.asarray(X)\n",
        "        else: X_arr = X\n",
        "        if len(X_arr) < self.get_n_splits(X_arr, y, groups): return\n",
        "        for train_idx, test_idx in super().split(X_arr, y, groups):\n",
        "            purged_train_idx = train_idx[train_idx < test_idx[0] - self.embargo]\n",
        "            if len(purged_train_idx) == 0: continue\n",
        "            yield purged_train_idx, test_idx\n",
        "\n",
        "def tune_model(model_cls, model_name: str, param_space, X, y, cv, gpu_params=None):\n",
        "    if gpu_params is None: gpu_params = {}\n",
        "    def objective(trial):\n",
        "        params = {}\n",
        "        for k, v_config in param_space.items():\n",
        "            if isinstance(v_config, list): params[k] = trial.suggest_categorical(k, v_config)\n",
        "            elif isinstance(v_config, tuple) and len(v_config) == 3 and v_config[2] == 'log': params[k] = trial.suggest_float(k, v_config[0], v_config[1], log=True)\n",
        "            elif isinstance(v_config, tuple) and len(v_config) == 2: params[k] = trial.suggest_float(k, v_config[0], v_config[1])\n",
        "            else: print(f\"Warning: Unsupported param_space config for {k}: {v_config} in {model_name}.\")\n",
        "\n",
        "        current_params = {**params, **gpu_params} # Merge Optuna params with fixed GPU params\n",
        "\n",
        "        if model_cls == CatBoostRegressor:\n",
        "            current_params['verbose'] = 0\n",
        "            if 'depth' in current_params and current_params['depth'] is not None: current_params['depth'] = int(current_params['depth'])\n",
        "\n",
        "        # For LGBM, n_estimators is often passed as 'n_rounds' in CV, but here it's a direct param.\n",
        "        # Max_depth = -1 is no limit for LGBM.\n",
        "        # For XGB, n_estimators is a direct param.\n",
        "\n",
        "        model = model_cls(**current_params, random_state=42)\n",
        "        try:\n",
        "            scores = cross_val_score(model, X, y, cv=cv, scoring='neg_mean_absolute_error', error_score='raise')\n",
        "        except Exception as e_cv_score:\n",
        "            print(f\"Debug: cross_val_score failed for {model_name} with params {current_params}. Error: {e_cv_score}\")\n",
        "            return np.nan\n",
        "        if len(scores) == 0 or np.all(np.isnan(scores)): return np.nan\n",
        "        return -np.nanmean(scores)\n",
        "\n",
        "    study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42, multivariate=True))\n",
        "    # Reduced n_trials for faster example, increase for real tuning\n",
        "    study.optimize(objective, n_trials=30, timeout=600, show_progress_bar=True)\n",
        "\n",
        "    best_params_from_study = {**study.best_params, **gpu_params} # Combine best Optuna params with fixed GPU params\n",
        "    if model_cls == CatBoostRegressor:\n",
        "        best_params_from_study['verbose'] = 0\n",
        "        if 'depth' in best_params_from_study and best_params_from_study['depth'] is not None: best_params_from_study['depth'] = int(best_params_from_study['depth'])\n",
        "\n",
        "    return model_cls(**best_params_from_study, random_state=42)\n",
        "\n",
        "\n",
        "def fit_models_on_gpu(X, y, asset_name_logging=\"\"):\n",
        "    cv = PurgedKFold(n_splits=8, embargo=10) # n_splits=5 for smaller datasets if needed\n",
        "    num_potential_splits = 0\n",
        "    if X.shape[0] >= cv.get_n_splits():\n",
        "        num_potential_splits = sum(1 for _ in cv.split(X, y))\n",
        "\n",
        "    # GPU parameters\n",
        "    xgb_gpu_params = {'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
        "    lgbm_gpu_params = {'device': 'gpu', 'gpu_platform_id': 0, 'gpu_device_id': 0} # Check platform/device IDs if multiple GPUs/platforms\n",
        "    catboost_gpu_params = {'task_type': 'GPU', 'devices': '0'}\n",
        "\n",
        "    model_configs = {\n",
        "        \"LGBM_GPU\": (lgb.LGBMRegressor, {\n",
        "            \"num_leaves\": [31, 63, 127], \"learning_rate\": (1e-3, 0.2, 'log'),\n",
        "            \"max_depth\": [-1, 6, 12], \"n_estimators\": [100, 200, 400] # Added n_estimators\n",
        "        }, lgbm_gpu_params),\n",
        "        \"CatBoost_GPU\": (CatBoostRegressor, {\n",
        "            \"depth\": [4, 6, 8], \"learning_rate\": (1e-3, 0.2, 'log'),\n",
        "            \"iterations\": [100, 200, 400] # iterations is n_estimators for CatBoost\n",
        "        }, catboost_gpu_params),\n",
        "        \"XGB_GPU\": (XGBRegressor, {\n",
        "            \"n_estimators\": [100, 200, 400], \"learning_rate\": (0.01, 0.2, 'log'),\n",
        "            \"max_depth\": [3, 6, 9]\n",
        "        }, xgb_gpu_params),\n",
        "        \"RF_CPU\": (RandomForestRegressor, { # RandomForest remains on CPU\n",
        "            \"n_estimators\": [100, 200, 300], \"max_depth\": [8, 16, None]\n",
        "        }, {}), # Empty dict for gpu_params for RF\n",
        "    }\n",
        "\n",
        "    if num_potential_splits == 0:\n",
        "        print(f\"Warning [{asset_name_logging}]: PurgedKFold yields no valid splits. Fitting models with default parameters (GPU where applicable).\")\n",
        "        fitted_models_fallback = {}\n",
        "        for name, (model_cls, _, current_gpu_params) in model_configs.items():\n",
        "            print(f\"  Fitting fallback {name} for asset {asset_name_logging}...\")\n",
        "            try:\n",
        "                params_with_gpu = current_gpu_params.copy()\n",
        "                if model_cls == CatBoostRegressor: params_with_gpu['verbose'] = 0\n",
        "                # Add default n_estimators if not in gpu_params\n",
        "                if model_cls in [XGBRegressor, lgb.LGBMRegressor, RandomForestRegressor] and 'n_estimators' not in params_with_gpu:\n",
        "                    if 'n_estimators' in model_configs[name][1]: # Check if defined in space\n",
        "                         params_with_gpu['n_estimators'] = model_configs[name][1]['n_estimators'][0] # take first option\n",
        "                    else: params_with_gpu['n_estimators'] = 100 # a general default\n",
        "                if model_cls == CatBoostRegressor and 'iterations' not in params_with_gpu:\n",
        "                     if 'iterations' in model_configs[name][1]:\n",
        "                         params_with_gpu['iterations'] = model_configs[name][1]['iterations'][0]\n",
        "                     else: params_with_gpu['iterations'] = 100\n",
        "\n",
        "\n",
        "                model_instance = model_cls(**params_with_gpu, random_state=42)\n",
        "                model_instance.fit(X, y)\n",
        "                fitted_models_fallback[name] = model_instance\n",
        "            except Exception as e_fit_fb:\n",
        "                print(f\"    ✗ Error fitting fallback {name} for asset {asset_name_logging}: {e_fit_fb}\")\n",
        "                traceback.print_exc()\n",
        "        return fitted_models_fallback\n",
        "\n",
        "    fitted_models = {}\n",
        "    for name, (model_cls, param_space, current_gpu_params) in model_configs.items():\n",
        "        print(f\"  Tuning {name} for asset {asset_name_logging}...\")\n",
        "        try:\n",
        "            tuned_model_instance = tune_model(model_cls, name, param_space, X, y, cv, gpu_params=current_gpu_params)\n",
        "            # tune_model returns an unfitted model with best_params (including GPU params)\n",
        "            print(f\"    Final fitting for {name} on asset {asset_name_logging} with params: {tuned_model_instance.get_params()}\")\n",
        "            tuned_model_instance.fit(X, y) # Fit on the full dataset\n",
        "            fitted_models[name] = tuned_model_instance\n",
        "            print(f\"    ✓ Tuned and fitted {name} for asset {asset_name_logging}.\")\n",
        "        except Exception as e_tune_fit:\n",
        "            print(f\"    ✗ Error during tuning or final fit for {name} on asset {asset_name_logging}: {type(e_tune_fit).__name__} - {e_tune_fit}\")\n",
        "            traceback.print_exc()\n",
        "            print(f\"    Attempting to fit {name} with default parameters (and GPU where applicable) for asset {asset_name_logging}.\")\n",
        "            try:\n",
        "                default_params_with_gpu = current_gpu_params.copy()\n",
        "                if model_cls == CatBoostRegressor: default_params_with_gpu['verbose'] = 0\n",
        "                # Add default n_estimators if not in gpu_params\n",
        "                if model_cls in [XGBRegressor, lgb.LGBMRegressor, RandomForestRegressor] and 'n_estimators' not in default_params_with_gpu:\n",
        "                    default_params_with_gpu['n_estimators'] = 100 # A general default\n",
        "                if model_cls == CatBoostRegressor and 'iterations' not in default_params_with_gpu:\n",
        "                    default_params_with_gpu['iterations'] = 100\n",
        "\n",
        "                default_model = model_cls(**default_params_with_gpu, random_state=42)\n",
        "                default_model.fit(X, y)\n",
        "                fitted_models[name] = default_model\n",
        "                print(f\"    ✓ Fitted {name} with default parameters for asset {asset_name_logging}.\")\n",
        "            except Exception as e_def_fit:\n",
        "                print(f\"      ✗ Failed default fit {name} for {asset_name_logging}: {e_def_fit}\")\n",
        "                traceback.print_exc()\n",
        "    return fitted_models\n",
        "\n",
        "# ──────────────── Train per Asset (Using GPU function) ────────────────\n",
        "print(\"\\n--- Training models per asset (GPU where applicable) ---\")\n",
        "asset_models = {}\n",
        "for a in datasets.keys():\n",
        "    print(f\"\\n--- Processing asset for training: {a} ---\")\n",
        "    try:\n",
        "        if a not in datasets or not isinstance(datasets[a], tuple) or len(datasets[a]) != 2:\n",
        "            print(f\"  ✗ {a} failed: Dataset not found correctly in 'datasets' dict. Skipping.\")\n",
        "            asset_models[a] = {}\n",
        "            continue\n",
        "        X_current, y_current = datasets[a]\n",
        "        if not isinstance(X_current, np.ndarray) or X_current.ndim != 2 or X_current.shape[0] == 0:\n",
        "            print(f\"  ✗ {a} failed: X data is empty or invalid. Shape: {getattr(X_current, 'shape', 'N/A')}. Skipping.\")\n",
        "            asset_models[a] = {}\n",
        "            continue\n",
        "        if not isinstance(y_current, np.ndarray) or y_current.ndim != 1 or y_current.shape[0] == 0:\n",
        "            print(f\"  ✗ {a} failed: y data is empty or invalid. Shape: {getattr(y_current, 'shape', 'N/A')}. Skipping.\")\n",
        "            asset_models[a] = {}\n",
        "            continue\n",
        "\n",
        "        print(f\"Fitting models for {a} … (X shape: {X_current.shape}, y shape: {y_current.shape})\")\n",
        "        asset_models[a] = fit_models_on_gpu(X_current, y_current, asset_name_logging=a) # Call GPU version\n",
        "\n",
        "        if asset_models.get(a): print(f\"  ✓ {a} processing done. Models: {list(asset_models[a].keys())}\")\n",
        "        else: print(f\"  ✗ {a} processing done, but no models were successfully fitted.\"); asset_models[a] = {}\n",
        "    except Exception as e_asset_loop:\n",
        "        print(f\"  ✗ {a} failed in main asset loop → {type(e_asset_loop).__name__}: {e_asset_loop}\"); traceback.print_exc(); asset_models[a] = {}\n",
        "\n",
        "print(\"\\n=== Trained Models Overview: ===\")\n",
        "if not asset_models: print(\"No assets processed or no models trained.\")\n",
        "else:\n",
        "    all_empty = True\n",
        "    for asset_k, models_v in asset_models.items():\n",
        "        if models_v and isinstance(models_v, dict) and models_v.keys(): print(f\"  Asset {asset_k}: Models - {list(models_v.keys())}\"); all_empty=False\n",
        "        else: print(f\"  Asset {asset_k}: No models successfully fitted.\")\n",
        "    if all_empty: print(\"No models were successfully trained for any asset.\")\n",
        "\n",
        "\n",
        "# ───────────────────────────────── Backtesting Section ─────────────────────────────────\n",
        "print(\"\\n--- Preparing for Backtesting ---\")\n",
        "try: from IPython.display import display\n",
        "except ImportError: display = print\n",
        "\n",
        "def prepare_live_df(asset: str) -> pd.DataFrame:\n",
        "    print(f\"\\nPreparing live dataframe for asset: {asset}\")\n",
        "    if asset not in raw_bt or raw_bt[asset].is_empty():\n",
        "        print(f\"Critical error: Raw backtest data for asset {asset} is missing or empty.\"); return pd.DataFrame()\n",
        "\n",
        "    feats_pl = engineer_features(raw_bt[asset], asset)\n",
        "    if feats_pl.is_empty():\n",
        "        print(f\"Warning: Feature engineering for backtest data of asset {asset} resulted in an empty DataFrame. Backtest may fail or be meaningless.\")\n",
        "        # Depending on strategy, might want to return empty df or df with only OHLCV if that's handled\n",
        "        # For now, let's return an empty df to signal issue clearly to backtester\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    feats_pd = feats_pl.to_pandas().reset_index(drop=True)\n",
        "    live_pd_full = raw_bt[asset].to_pandas()\n",
        "\n",
        "    if live_pd_full.empty: print(f\"Critical Error: Raw backtest data for {asset} empty.\"); return pd.DataFrame()\n",
        "\n",
        "    # Align based on potentially shorter feats_pd\n",
        "    if not feats_pd.empty: live_pd_aligned = live_pd_full.iloc[-len(feats_pd):].copy().reset_index(drop=True)\n",
        "    else: # This case should be rare if feats_pl check above is handled\n",
        "        print(f\"Warning: Features DataFrame is empty for {asset} during alignment. Using full live data for OHLCV, but features will be missing.\");\n",
        "        live_pd_aligned = live_pd_full.copy().reset_index(drop=True)\n",
        "\n",
        "    pref = PREFIX[asset]\n",
        "    rename_map = {}\n",
        "    for col_std, col_orig_pattern in [(\"Open\",f\"{pref}_open\"), (\"High\",f\"{pref}_high\"), (\"Low\",f\"{pref}_low\"), (\"Close\",f\"{pref}_close\"), (\"Volume\",f\"{pref}_volume\")]:\n",
        "        if col_orig_pattern in live_pd_aligned.columns: rename_map[col_orig_pattern] = col_std\n",
        "        elif col_std.lower() in live_pd_aligned.columns: rename_map[col_std.lower()] = col_std # Fallback to generic lower\n",
        "\n",
        "    live_pd_renamed = live_pd_aligned.rename(columns=rename_map)\n",
        "    final_df = live_pd_renamed.copy()\n",
        "\n",
        "    if not feats_pd.empty: # Add features if they exist\n",
        "        for col in feats_pd.columns:\n",
        "            if col in final_df.columns and col not in ['Open','High','Low','Close','Volume']: # Avoid overwriting standard OHLCV if a feature has same name\n",
        "                print(f\"Warning: Feature col '{col}' for {asset} clashes. Overwriting.\")\n",
        "            final_df[col] = feats_pd[col].values # Assign features\n",
        "\n",
        "    if 'ts' in final_df.columns:\n",
        "        final_df['timestamp_dt'] = pd.to_datetime(final_df['ts'])\n",
        "        final_df = final_df.set_index('timestamp_dt', drop=True)\n",
        "    else: print(f\"Warning: 'ts' col not for DatetimeIndex in {asset} backtest data.\")\n",
        "\n",
        "    required_bt_cols = [\"Open\", \"High\", \"Low\", \"Close\"]\n",
        "    missing_bt_cols = [col for col in required_bt_cols if col not in final_df.columns]\n",
        "    if missing_bt_cols: raise ValueError(f\"Asset {asset}: Missing OHLC for Backtesting.py: {missing_bt_cols}. Cols: {final_df.columns.tolist()}\")\n",
        "\n",
        "    # Ensure OHLC are numeric and not all NaN\n",
        "    for col in required_bt_cols:\n",
        "        final_df[col] = pd.to_numeric(final_df[col], errors='coerce')\n",
        "    if final_df[required_bt_cols].isnull().all().all(): # If ALL OHLC data is NaN\n",
        "        print(f\"Critical: All OHLC data for {asset} is NaN. Returning empty DataFrame.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    print(f\"Prepared live df for {asset}. Shape: {final_df.shape}, Index: {type(final_df.index)}\")\n",
        "    return final_df\n",
        "\n",
        "def build_meta_strategy(asset):\n",
        "    if not asset_models.get(asset) or not isinstance(asset_models[asset], dict) or not asset_models[asset].keys():\n",
        "        print(f\"Warning: No models for {asset}. Cannot build strategy.\"); return None\n",
        "\n",
        "    current_asset_models_dict = asset_models[asset]\n",
        "    # Filter out any models that might be None if fitting failed for them\n",
        "    valid_models = {name: model for name, model in current_asset_models_dict.items() if model is not None}\n",
        "    if not valid_models:\n",
        "        print(f\"Warning: No valid (non-None) models found for asset {asset} after filtering. Cannot build strategy.\"); return None\n",
        "\n",
        "    weights = {k: 1/len(valid_models) for k in valid_models}\n",
        "\n",
        "    class MetaVoteStrategy(Strategy):\n",
        "        # Closure variables from outer scope\n",
        "        _models_to_use_cl = valid_models\n",
        "        _model_weights_cl = weights\n",
        "        _trade_threshold_cl = 0.00 # Example: 0.00 for any signal, adjust as needed (e.g. 0.001 for 0.1% predicted return)\n",
        "        _feature_names_list_cl = None\n",
        "\n",
        "        def init(self):\n",
        "            self.models_to_use = self._models_to_use_cl\n",
        "            self.model_weights = self._model_weights_cl\n",
        "            self.trade_threshold = self._trade_threshold_cl\n",
        "\n",
        "            # One-time setup for feature names\n",
        "            if MetaVoteStrategy._feature_names_list_cl is None and self.models_to_use:\n",
        "                # Try to get feature names from the first model\n",
        "                first_model_name = list(self.models_to_use.keys())[0]\n",
        "                first_model = self.models_to_use[first_model_name]\n",
        "\n",
        "                if hasattr(first_model, 'feature_name_'): # LightGBM\n",
        "                    MetaVoteStrategy._feature_names_list_cl = first_model.feature_name_()\n",
        "                elif hasattr(first_model, 'feature_names_in_'): # Scikit-learn, CatBoost (often)\n",
        "                    MetaVoteStrategy._feature_names_list_cl = first_model.feature_names_in_\n",
        "                elif hasattr(first_model, 'get_booster') and hasattr(first_model.get_booster(), 'feature_names'): # XGBoost\n",
        "                     MetaVoteStrategy._feature_names_list_cl = first_model.get_booster().feature_names\n",
        "\n",
        "                if MetaVoteStrategy._feature_names_list_cl is None: # Fallback\n",
        "                    ohlcv_std = {'Open', 'High', 'Low', 'Close', 'Volume', 'ts', 'timestamp_dt'}\n",
        "                    # Assuming self.data.df is available at init, might not be fully populated.\n",
        "                    # This might be better done in the first call to _pred if df structure is not fixed at init.\n",
        "                    # For now, this is a best guess.\n",
        "                    if self.data and hasattr(self.data, 'df') and not self.data.df.empty:\n",
        "                         MetaVoteStrategy._feature_names_list_cl = [col for col in self.data.df.columns if col not in ohlcv_std]\n",
        "                    else: # Cannot determine feature names yet\n",
        "                        print(\"Warning: Could not determine feature names at init in MetaVoteStrategy.\")\n",
        "            self.feature_names_list_ = MetaVoteStrategy._feature_names_list_cl\n",
        "\n",
        "\n",
        "        def _pred(self):\n",
        "            if not self.feature_names_list_:\n",
        "                # Attempt to set up feature names if not done in init (e.g. self.data.df wasn't ready)\n",
        "                if MetaVoteStrategy._feature_names_list_cl is None and self.models_to_use:\n",
        "                    first_model_name = list(self.models_to_use.keys())[0]; first_model = self.models_to_use[first_model_name]\n",
        "                    if hasattr(first_model, 'feature_name_'): MetaVoteStrategy._feature_names_list_cl = first_model.feature_name_()\n",
        "                    elif hasattr(first_model, 'feature_names_in_'): MetaVoteStrategy._feature_names_list_cl = first_model.feature_names_in_\n",
        "                    elif hasattr(first_model, 'get_booster') and hasattr(first_model.get_booster(), 'feature_names'): MetaVoteStrategy._feature_names_list_cl = first_model.get_booster().feature_names\n",
        "                    if MetaVoteStrategy._feature_names_list_cl is None:\n",
        "                        ohlcv_std = {'Open', 'High', 'Low', 'Close', 'Volume', 'ts', 'timestamp_dt'}\n",
        "                        MetaVoteStrategy._feature_names_list_cl = [col for col in self.data.df.columns if col not in ohlcv_std]\n",
        "                self.feature_names_list_ = MetaVoteStrategy._feature_names_list_cl\n",
        "                if not self.feature_names_list_:\n",
        "                    print(\"CRITICAL Error: Still cannot determine feature names for prediction in MetaVoteStrategy _pred.\")\n",
        "                    return 0\n",
        "\n",
        "            try:\n",
        "                current_feature_values = [self.data.df[fn].iloc[-1] for fn in self.feature_names_list_]\n",
        "            except KeyError as e: print(f\"KeyError in _pred for feature {e}. Available: {self.data.df.columns.tolist()}\"); return 0\n",
        "            except IndexError: print(\"IndexError in _pred, likely empty data slice.\"); return 0\n",
        "\n",
        "            if np.isnan(current_feature_values).any(): return 0\n",
        "\n",
        "            feature_array = np.array(current_feature_values).reshape(1, -1)\n",
        "            preds = {}\n",
        "            for n, m in self.models_to_use.items():\n",
        "                try: preds[n] = m.predict(feature_array)[0]\n",
        "                except Exception as e_p: print(f\"Err predicting {n}: {e_p}. Assuming no change.\"); preds[n] = self.data.Close[-1]\n",
        "\n",
        "            last_price = self.data.Close[-1]\n",
        "            if last_price == 0 or np.isnan(last_price): return 0\n",
        "\n",
        "            rets = {n: (p - last_price) / last_price if not np.isnan(p) else 0 for n, p in preds.items()}\n",
        "            return sum(self.model_weights[n] * rets[n] for n in rets)\n",
        "\n",
        "        def next(self):\n",
        "            if not self.models_to_use: return\n",
        "            # Heuristic buffer for TA stability in features.\n",
        "            # If features are precomputed and NaNs handled, this might be less critical.\n",
        "            if len(self.data.Close) < max(60, lags_from_config if 'lags_from_config' in globals() else 5) + 5 : # Ensure enough data for largest window + lags\n",
        "                return\n",
        "            signal = self._pred()\n",
        "            if np.isnan(signal): signal = 0 # Handle NaN signal as neutral\n",
        "\n",
        "            if signal > self.trade_threshold and not self.position.is_long:\n",
        "                self.position.close(); self.buy()\n",
        "            elif signal < -self.trade_threshold and not self.position.is_short:\n",
        "                self.position.close(); self.sell()\n",
        "    return MetaVoteStrategy\n",
        "\n",
        "# ---------- run back‑test per asset ----------\n",
        "print(\"\\n--- Running Backtests ---\")\n",
        "stats = {}\n",
        "equity_curves = {}\n",
        "# Use keys from asset_models as these are the assets for which training was attempted\n",
        "# This ensures we only try to backtest assets that might have models.\n",
        "assets_to_backtest = list(asset_models.keys())\n",
        "\n",
        "for asset_key_bt in assets_to_backtest:\n",
        "    print(f\"\\n--- Running Backtest for: {asset_key_bt} ---\")\n",
        "    # Check again if models actually exist and are valid for this asset\n",
        "    if not asset_models.get(asset_key_bt) or not any(asset_models[asset_key_bt].values()):\n",
        "        print(f\"Skipping backtest for {asset_key_bt}: No valid models were trained or found.\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        live_df_prepared = prepare_live_df(asset_key_bt)\n",
        "        if live_df_prepared.empty or not isinstance(live_df_prepared.index, pd.DatetimeIndex):\n",
        "            print(f\"Skipping backtest for {asset_key_bt}: Live DataFrame unusable.\"); continue\n",
        "        if live_df_prepared[['Open', 'High', 'Low', 'Close']].isnull().values.any():\n",
        "            print(f\"Skipping backtest for {asset_key_bt}: Live DataFrame OHLC NaNs.\"); continue\n",
        "\n",
        "        StratBuilder = build_meta_strategy(asset_key_bt)\n",
        "        if StratBuilder is None: print(f\"Skipping {asset_key_bt}: Strategy not built.\"); continue\n",
        "\n",
        "        bt_instance = Backtest(live_df_prepared, StratBuilder, cash=1_000_000, commission=0.0015, exclusive_orders=False)\n",
        "        s_result = bt_instance.run()\n",
        "        stats[asset_key_bt] = s_result\n",
        "        if '_equity_curve' in s_result and not s_result['_equity_curve'].empty:\n",
        "             equity_curves[asset_key_bt] = s_result['_equity_curve']['Equity']\n",
        "        # Optional plot: bt_instance.plot(filename=f\"backtest_{asset_key_bt}.html\", open_browser=False)\n",
        "    except Exception as e_bt_general:\n",
        "        print(f\"Error during backtest for {asset_key_bt}: {type(e_bt_general).__name__} - {e_bt_general}\"); traceback.print_exc()\n",
        "\n",
        "# ---------- summary table & equity curves ----------\n",
        "if stats:\n",
        "    summary_data = {}\n",
        "    for a, s_val in stats.items():\n",
        "        if hasattr(s_val, 'get'): # Handles Series/dict from Backtesting.py\n",
        "            summary_data[a] = {\n",
        "                \"APR\": s_val.get('Return (Ann.) [%]', np.nan),\n",
        "                \"Sharpe\": s_val.get('Sharpe Ratio', np.nan),\n",
        "                \"MaxDD\": s_val.get('Max. Drawdown [%]', np.nan),\n",
        "                \"WinRate\": s_val.get('Win Rate [%]', np.nan) / 100 if pd.notna(s_val.get('Win Rate [%]')) else np.nan,\n",
        "                \"Trades\": s_val.get('# Trades', 0),\n",
        "                \"Exposure\": s_val.get('Exposure Time [%]', np.nan) / 100 if pd.notna(s_val.get('Exposure Time [%]')) else np.nan\n",
        "            }\n",
        "    summary = pd.DataFrame(summary_data).T.round(3)\n",
        "    print(\"\\n=== Hold‑out performance ===\")\n",
        "    if not summary.empty: display(summary)\n",
        "    else: print(\"No summary data to display.\")\n",
        "else: print(\"\\nNo backtesting statistics were generated.\")\n",
        "\n",
        "if equity_curves:\n",
        "    plt.figure(figsize=(12,7))\n",
        "    for asset_label_eq, curve_data in equity_curves.items():\n",
        "        if not curve_data.empty: plt.plot(curve_data.index, curve_data, label=asset_label_eq.upper())\n",
        "    plt.title(\"Equity Curves ‑ Hold‑out Period\"); plt.xlabel(\"Date\"); plt.ylabel(\"Equity ($)\"); plt.legend(); plt.grid(True);\n",
        "    # Save plot instead of showing if in non-interactive environment\n",
        "    # plt.savefig(\"equity_curves.png\")\n",
        "    plt.show()\n",
        "else: print(\"No equity curves to plot.\")\n",
        "\n",
        "print(\"\\n--- Script End ---\")\n",
        "\n",
        "# Optional: Clean up the global temporary directory\n",
        "# import shutil\n",
        "# if '_temp_dir_for_dummy_files' in globals() and os.path.exists(_temp_dir_for_dummy_files) and _temp_dir_for_dummy_files != \".\":\n",
        "#     try:\n",
        "#         print(f\"INFO: Removing temporary directory: {_temp_dir_for_dummy_files}\")\n",
        "#         shutil.rmtree(_temp_dir_for_dummy_files)\n",
        "#     except Exception as e_shutil:\n",
        "#         print(f\"Warning: Could not remove temporary directory {_temp_dir_for_dummy_files}: {e_shutil}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uo76SAZyx-f"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwCIMwBnyx74"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcMWHLkEyx5R"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dh3yO-QVyx0W"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "INitialize and load in packages and data"
      ],
      "metadata": {
        "id": "5cL_L2fwU_F2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install polars pandas scikit-learn pyarrow catboost optuna lightgbm xgboost ta Backtesting matplotlib\n",
        "import polars as pl\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler # Not used in this version, but kept if needed later\n",
        "from sklearn.metrics import classification_report, mean_absolute_error # classification_report not used\n",
        "import ta\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "import lightgbm as lgb\n",
        "import optuna\n",
        "from backtesting import Backtest, Strategy\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import tempfile\n",
        "import traceback\n",
        "\n",
        "# --- Configuration Section ---\n",
        "# Define your intended TRAIN_PATHS and BACKTEST_PATHS.\n",
        "# If your files are located elsewhere, use absolute paths here.\n",
        "INTENDED_TRAIN_PATHS = {\n",
        "    \"ada\": \"ml_training_ada.csv\",\n",
        "    \"btc\": \"ml_training_btc.csv\",\n",
        "    \"eth\": \"ml_training_eth.csv\",\n",
        "    \"sol\": \"ml_training_solana.csv\",\n",
        "}\n",
        "\n",
        "INTENDED_BACKTEST_PATHS = {\n",
        "    \"ada\": \"2024_to_april_2025_ada_data.csv\",\n",
        "    \"btc\": \"2024_to_april_2025_btc_data.csv\",\n",
        "    \"eth\": \"2024_to_april_2025_eth_data.csv\",\n",
        "    \"sol\": \"2024_to_april_2025_solana_data.csv\",\n",
        "}\n",
        "\n",
        "TARGET_COL = {\n",
        "    \"ada\": \"target_next_close_ada\",\n",
        "    \"btc\": \"target_next_close_btc\",\n",
        "    \"eth\": \"target_next_close_eth\",\n",
        "    \"sol\": \"target_next_close_solana\",\n",
        "}\n",
        "\n",
        "PREFIX = {\n",
        "    \"ada\": \"ada\",\n",
        "    \"btc\": \"btc\",\n",
        "    \"eth\": \"eth\",\n",
        "    \"sol\": \"solana\",\n",
        "}\n"
      ],
      "metadata": {
        "id": "0xs3lRBtUcm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Global temporary directory for dummy files ---\n",
        "try:\n",
        "    _temp_dir_for_dummy_files = tempfile.mkdtemp(prefix=\"pydummy_ml_data_\")\n",
        "    print(f\"INFO: Dummy files, if created, will be stored in: {_temp_dir_for_dummy_files}\")\n",
        "except Exception as e:\n",
        "    print(f\"WARNING: Could not create a global temporary directory ({e}). Falling back to current directory for dummy files, which might fail if read-only.\")\n",
        "    _temp_dir_for_dummy_files = \".\"\n",
        "\n",
        "\n",
        "# --- Dummy file creation function (OSError Fix) ---\n",
        "def create_dummy_csv_if_not_exists(asset_label, original_path_from_config, is_training, base_temp_dir):\n",
        "    \"\"\"\n",
        "    Checks if a file exists at original_path_from_config.\n",
        "    If not, creates a dummy CSV in base_temp_dir and returns its path.\n",
        "    Otherwise, returns original_path_from_config.\n",
        "    \"\"\"\n",
        "    if os.path.exists(original_path_from_config):\n",
        "        print(f\"INFO: File found at {original_path_from_config}. Using existing file for asset '{asset_label}'.\")\n",
        "        return original_path_from_config\n",
        "\n",
        "    original_filename = os.path.basename(original_path_from_config)\n",
        "    dummy_file_path = os.path.join(base_temp_dir, original_filename)\n",
        "    print(f\"WARNING: File {original_path_from_config} for asset '{asset_label}' not found. Creating a dummy CSV at: {dummy_file_path}\")\n",
        "\n",
        "    num_rows = 500 # Increased rows for more stable TA in dummy data\n",
        "    if asset_label not in PREFIX:\n",
        "        raise ValueError(f\"Asset label '{asset_label}' not found in PREFIX. Cannot generate dummy columns.\")\n",
        "    if is_training and asset_label not in TARGET_COL:\n",
        "         raise ValueError(f\"Asset label '{asset_label}' not found in TARGET_COL. Cannot generate dummy target column.\")\n",
        "\n",
        "    asset_p = PREFIX[asset_label]\n",
        "    dummy_data = {}\n",
        "\n",
        "    if is_training:\n",
        "        dummy_data['timestamp'] = pd.to_datetime(pd.date_range(start='2022-01-01', periods=num_rows, freq='T')).strftime('%Y-%m-%dT%H:%M:%S.%f')\n",
        "    else:\n",
        "        datetime_series = pd.to_datetime(pd.date_range(start='2024-01-01', periods=num_rows, freq='T'))\n",
        "        dummy_data['date_only'] = datetime_series.strftime('%Y-%m-%d')\n",
        "        dummy_data['time_only'] = datetime_series.strftime('%H:%M:%S')\n",
        "\n",
        "    # More realistic OHLCV data\n",
        "    price_open = np.abs(np.random.normal(loc=100, scale=20, size=num_rows))\n",
        "    price_open[0] = 100 # Start at a defined point\n",
        "    price_open = np.maximum(price_open, 1) # Ensure positive prices\n",
        "\n",
        "    dummy_data[f'{asset_p}_open'] = price_open\n",
        "    dummy_data[f'{asset_p}_high'] = price_open + np.abs(np.random.normal(loc=2, scale=1, size=num_rows))\n",
        "    dummy_data[f'{asset_p}_low'] = price_open - np.abs(np.random.normal(loc=2, scale=1, size=num_rows))\n",
        "    dummy_data[f'{asset_p}_low'] = np.minimum(dummy_data[f'{asset_p}_low'], dummy_data[f'{asset_p}_open']) # Ensure low <= open\n",
        "    dummy_data[f'{asset_p}_low'] = np.maximum(dummy_data[f'{asset_p}_low'], 0.1) # Ensure positive low\n",
        "    dummy_data[f'{asset_p}_high'] = np.maximum(dummy_data[f'{asset_p}_high'], dummy_data[f'{asset_p}_open']) # Ensure high >= open\n",
        "\n",
        "    # Close price is within low and high\n",
        "    dummy_data[f'{asset_p}_close'] = np.random.uniform(dummy_data[f'{asset_p}_low'], dummy_data[f'{asset_p}_high'])\n",
        "    dummy_data[f'{asset_p}_volume'] = np.abs(np.random.normal(loc=1000, scale=500, size=num_rows)) + 100 # Ensure positive volume\n",
        "\n",
        "    if is_training:\n",
        "        target_col_name = TARGET_COL[asset_label]\n",
        "        dummy_data[target_col_name] = dummy_data[f'{asset_p}_close'] * (1 + np.random.normal(loc=0, scale=0.01, size=num_rows))\n",
        "\n",
        "    df = pd.DataFrame(dummy_data)\n",
        "    try:\n",
        "        os.makedirs(os.path.dirname(dummy_file_path), exist_ok=True)\n",
        "        df.to_csv(dummy_file_path, index=False)\n",
        "        print(f\"INFO: Successfully created dummy file for asset '{asset_label}': {dummy_file_path}\")\n",
        "        return dummy_file_path\n",
        "    except OSError as e:\n",
        "        print(f\"CRITICAL ERROR: Could not write dummy CSV for asset '{asset_label}' to {dummy_file_path}. OS error: {e}\")\n",
        "        raise\n",
        "\n",
        "# --- Paths to be used by the script (potentially updated to dummy file paths) ---\n",
        "ACTUAL_TRAIN_PATHS = INTENDED_TRAIN_PATHS.copy()\n",
        "ACTUAL_BACKTEST_PATHS = INTENDED_BACKTEST_PATHS.copy()\n",
        "\n",
        "print(\"\\n--- Checking for training data files and creating dummies if necessary ---\")\n",
        "for asset_key in list(ACTUAL_TRAIN_PATHS.keys()):\n",
        "    original_path = ACTUAL_TRAIN_PATHS[asset_key]\n",
        "    ACTUAL_TRAIN_PATHS[asset_key] = create_dummy_csv_if_not_exists(\n",
        "        asset_key, original_path, is_training=True, base_temp_dir=_temp_dir_for_dummy_files\n",
        "    )\n",
        "\n",
        "print(\"\\n--- Checking for backtest data files and creating dummies if necessary ---\")\n",
        "for asset_key in list(ACTUAL_BACKTEST_PATHS.keys()):\n",
        "    original_path = ACTUAL_BACKTEST_PATHS[asset_key]\n",
        "    ACTUAL_BACKTEST_PATHS[asset_key] = create_dummy_csv_if_not_exists(\n",
        "        asset_key, original_path, is_training=False, base_temp_dir=_temp_dir_for_dummy_files\n",
        "    )\n",
        "\n",
        "# --- Data Loading Functions ---\n",
        "def load_training(asset):\n",
        "    print(f\"Loading training data for {asset} from: {ACTUAL_TRAIN_PATHS[asset]}\")\n",
        "    return (\n",
        "        pl.read_csv(\n",
        "            ACTUAL_TRAIN_PATHS[asset],\n",
        "            infer_schema_length=100_000,\n",
        "            truncate_ragged_lines=True # Added to handle ragged CSV lines\n",
        "        )\n",
        "        .with_columns(\n",
        "            pl.col(\"timestamp\")\n",
        "              .str.strptime(pl.Datetime, format=\"%Y-%m-%dT%H:%M:%S%.f\", strict=False)\n",
        "              .alias(\"ts\")\n",
        "        )\n",
        "        .sort(\"ts\")\n",
        "    )\n",
        "\n",
        "def load_backtest(asset):\n",
        "    print(f\"Loading backtest data for {asset} from: {ACTUAL_BACKTEST_PATHS[asset]}\")\n",
        "    return (\n",
        "        pl.read_csv(\n",
        "            ACTUAL_BACKTEST_PATHS[asset],\n",
        "            infer_schema_length=100_000,\n",
        "            truncate_ragged_lines=True # Added to handle ragged CSV lines\n",
        "        )\n",
        "        .with_columns(\n",
        "            (\n",
        "                pl.concat_str([pl.col(\"date_only\"), pl.lit(\" \"), pl.col(\"time_only\")])\n",
        "                .str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\", strict=False)\n",
        "            ).alias(\"ts\")\n",
        "        )\n",
        "        .sort(\"ts\")\n",
        "    )\n",
        "\n",
        "print(\"\\n--- Loading data ---\")\n",
        "raw_train = {a: load_training(a)  for a in ACTUAL_TRAIN_PATHS}\n",
        "raw_bt    = {a: load_backtest(a)  for a in ACTUAL_BACKTEST_PATHS}\n",
        "\n",
        "\n",
        "# --- Feature Engineering ---\n",
        "def engineer_features(df: pl.DataFrame, asset: str, lags: int = 5) -> pl.DataFrame:\n",
        "    if df.is_empty():\n",
        "        print(f\"Warning: Input DataFrame for engineer_features for asset '{asset}' is empty. Returning empty DataFrame.\")\n",
        "        return pl.DataFrame()\n",
        "\n",
        "    pref = PREFIX[asset]\n",
        "    ohlcv_cols_mapping = {}\n",
        "    rename_map_for_ta = {} # This will map original column names to \"open\", \"high\", \"low\", \"close\", \"volume\"\n",
        "\n",
        "    for col_type in [\"open\", \"high\", \"low\", \"close\", \"volume\"]:\n",
        "        prefixed_name = f\"{pref}_{col_type}\"\n",
        "        if prefixed_name in df.columns:\n",
        "            rename_map_for_ta[prefixed_name] = col_type\n",
        "        elif col_type in df.columns: # Fallback to generic name\n",
        "            rename_map_for_ta[col_type] = col_type\n",
        "\n",
        "    if not all(ct in rename_map_for_ta.values() for ct in [\"high\", \"low\", \"close\"]):\n",
        "        raise ValueError(f\"Asset {asset}: Could not find required HLC columns for TA. Found mappings: {rename_map_for_ta}. Available df columns: {df.columns}\")\n",
        "\n",
        "    base = df.select(list(rename_map_for_ta.keys())).rename(rename_map_for_ta)\n",
        "    p = base.to_pandas()\n",
        "\n",
        "    for col_name in [\"open\", \"high\", \"low\", \"close\", \"volume\"]:\n",
        "        if col_name in p.columns:\n",
        "            p[col_name] = pd.to_numeric(p[col_name], errors='coerce')\n",
        "\n",
        "    p.fillna(method='ffill', inplace=True) # Fill NaNs before TA\n",
        "    p.fillna(method='bfill', inplace=True) # Fill remaining NaNs at the beginning\n",
        "\n",
        "    if p[\"close\"].isnull().all() or p[\"high\"].isnull().all() or p[\"low\"].isnull().all():\n",
        "        print(f\"Warning: Asset {asset} - HLC columns are all NaN after processing. TA features will be NaN.\")\n",
        "        ta_feature_names = [\"rsi14\", \"stochk\", \"roc10\", \"macd\", \"bb_w\", \"atr14\", \"obv\"]\n",
        "        for ta_f in ta_feature_names: p[ta_f] = np.nan\n",
        "    else:\n",
        "        p[\"rsi14\"]  = ta.momentum.RSIIndicator(p[\"close\"], 14, fillna=True).rsi()\n",
        "        p[\"stochk\"] = ta.momentum.StochasticOscillator(p[\"high\"], p[\"low\"], p[\"close\"], 14, fillna=True).stoch()\n",
        "        p[\"roc10\"]  = ta.momentum.ROCIndicator(p[\"close\"], 10, fillna=True).roc()\n",
        "        p[\"macd\"]   = ta.trend.MACD(p[\"close\"], fillna=True).macd()\n",
        "        p[\"bb_w\"]   = ta.volatility.BollingerBands(p[\"close\"], fillna=True).bollinger_wband()\n",
        "        p[\"atr14\"]  = ta.volatility.AverageTrueRange(p[\"high\"], p[\"low\"], p[\"close\"], 14, fillna=True).average_true_range()\n",
        "        if \"volume\" in p.columns and not p[\"volume\"].isnull().all() and (p[\"volume\"] != 0).any() :\n",
        "            p[\"obv\"] = ta.volume.OnBalanceVolumeIndicator(p[\"close\"], p[\"volume\"], fillna=True).on_balance_volume()\n",
        "        else:\n",
        "            p[\"obv\"] = 0.0\n",
        "\n",
        "    f = pl.from_pandas(p)\n",
        "    cols_to_drop_after_ta = list(rename_map_for_ta.values())\n",
        "\n",
        "    for w in (20, 60, 120):\n",
        "        f = f.with_columns([\n",
        "            pl.col(\"close\").rolling_mean(w, min_periods=max(1, w//2)).alias(f\"sma{w}\"),\n",
        "            pl.col(\"close\").rolling_std(w, min_periods=max(1, w//2)).alias(f\"vol{w}\"),\n",
        "            (pl.col(\"high\") - pl.col(\"low\")).rolling_mean(w, min_periods=max(1, w//2)).alias(f\"hl_range{w}\")\n",
        "        ])\n",
        "    for l_val in range(1, lags + 1):\n",
        "        f = f.with_columns([\n",
        "            pl.col(\"close\").shift(l_val).alias(f\"close_lag{l_val}\"),\n",
        "            pl.col(\"rsi14\").shift(l_val).alias(f\"rsi_lag{l_val}\")\n",
        "        ])\n",
        "    return f.drop(cols_to_drop_after_ta)\n",
        "\n",
        "# --- Verify counts ---\n",
        "print(\"\\n--- Verifying feature counts ---\")\n",
        "for asset_key in ACTUAL_TRAIN_PATHS:\n",
        "    if asset_key in raw_train and not raw_train[asset_key].is_empty():\n",
        "        try:\n",
        "            engineered_df = engineer_features(raw_train[asset_key], asset_key)\n",
        "            if not engineered_df.is_empty():\n",
        "                feat_cols = engineered_df.columns\n",
        "                print(f\"{asset_key}: {len(feat_cols)} engineered vars → {feat_cols[:8]}\")\n",
        "            else:\n",
        "                print(f\"{asset_key}: Feature engineering resulted in an empty DataFrame.\")\n",
        "        except Exception as e_eng:\n",
        "            print(f\"Error engineering features for {asset_key}: {e_eng}\")\n",
        "            traceback.print_exc()\n",
        "    else:\n",
        "        print(f\"{asset_key}: Raw training data is empty or not loaded for feature count verification.\")\n",
        "\n",
        "\n",
        "# --- Build X / y ---\n",
        "def build_matrix(asset: str):\n",
        "    if asset not in raw_train or raw_train[asset].is_empty():\n",
        "        print(f\"Warning: Asset {asset} - Raw training data is empty for build_matrix.\")\n",
        "        return np.array([]).reshape(0,0), np.array([])\n",
        "\n",
        "    feats = engineer_features(raw_train[asset], asset)\n",
        "\n",
        "    if feats.is_empty():\n",
        "        print(f\"Warning: Asset {asset} - Features DataFrame is empty before combining with target.\")\n",
        "        return np.array([]).reshape(0,0), np.array([])\n",
        "\n",
        "    target_column_name = TARGET_COL[asset]\n",
        "    if target_column_name not in raw_train[asset].columns:\n",
        "        raise ValueError(f\"Target column '{target_column_name}' not found in raw_train['{asset}']. Available: {raw_train[asset].columns}\")\n",
        "\n",
        "    num_feat_cols = feats.width\n",
        "\n",
        "    # Combine features with target and then drop NaNs\n",
        "    # Ensure target is aligned with features (Polars does this by row number if not joining by key)\n",
        "    # It's safer to ensure row counts match or handle alignment explicitly if needed.\n",
        "    # Assuming raw_train[asset] and feats have compatible row structures before this point.\n",
        "    # If engineer_features drops rows, this needs careful handling.\n",
        "    # For simplicity, assuming engineer_features preserves row order and count relative to what's needed for target.\n",
        "\n",
        "    # A robust way: convert target to series, add as column, then drop nulls.\n",
        "    # Polars handles Series addition by matching row numbers if DataFrame shapes are compatible.\n",
        "    target_series = raw_train[asset][target_column_name]\n",
        "\n",
        "    # If feature engineering changed row count (e.g. due to shifts creating initial NaNs that are not dropped yet)\n",
        "    # we need to align target_series. This is complex.\n",
        "    # Simpler: assume engineer_features outputs df with same row count as input for now, or handles it.\n",
        "    # The drop_nulls after with_columns is key.\n",
        "\n",
        "    mat = feats.with_columns(target_series.alias(target_column_name)).drop_nulls()\n",
        "\n",
        "    if mat.is_empty():\n",
        "        print(f\"Warning: Asset {asset} - DataFrame is empty after combining features, target, and drop_nulls().\")\n",
        "        return np.array([]).reshape(0, num_feat_cols), np.array([])\n",
        "\n",
        "    X = mat.drop(target_column_name).to_numpy()\n",
        "    y = mat[target_column_name].to_numpy()\n",
        "    return X, y\n",
        "\n",
        "print(\"\\n--- Building datasets (X, y matrices) ---\")\n",
        "datasets = {a: build_matrix(a) for a in ACTUAL_TRAIN_PATHS.keys()}\n",
        "\n",
        "for a_key,(X_val,y_val) in datasets.items():\n",
        "    print(f\"{a_key}: {X_val.shape[1] if X_val.ndim == 2 and X_val.shape[0] > 0 else 0} features  |  samples {X_val.shape[0]}\")\n",
        "\n",
        "lengths = {a: y.shape[0] for a, (X, y) in datasets.items() if X.ndim == 2 and X.shape[0] > 0 and y.ndim == 1 and y.shape[0] > 0}\n",
        "if not lengths:\n",
        "    print(\"CRITICAL: All datasets are empty or have zero length after build_matrix. Cannot proceed with trimming or training.\")\n",
        "    min_n = 0\n",
        "else:\n",
        "    min_n = min(lengths.values())\n",
        "\n",
        "print(\"Samples before trim (from valid datasets):\", lengths)\n",
        "print(f\"→ Trimming all to {min_n} rows (smallest non-empty dataset's length)\")\n",
        "\n",
        "datasets_even = {}\n",
        "for a_d, (X_d, y_d) in datasets.items():\n",
        "    if X_d.ndim == 2 and X_d.shape[0] > 0 and y_d.ndim == 1 and y_d.shape[0] > 0 and X_d.shape[0] >= min_n and min_n > 0 :\n",
        "        datasets_even[a_d] = (X_d[-min_n:], y_d[-min_n:])\n",
        "    else:\n",
        "        datasets_even[a_d] = (X_d, y_d) # Keep as is if conditions not met\n",
        "\n",
        "for a_de, (X_e, y_e) in datasets_even.items():\n",
        "    print(f\"{a_de}: {X_e.shape[0]} samples, {X_e.shape[1] if X_e.ndim==2 and X_e.shape[0]>0 else 0} features after trimming attempt\")\n",
        "\n",
        "datasets = datasets_even\n"
      ],
      "metadata": {
        "id": "m0uvbeOYUcja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Tuning"
      ],
      "metadata": {
        "id": "jfKTZROvVCa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ───────────────────────────────────────── 2. Model Tuning ─────────────────────────────────────────\n",
        "print(\"\\n--- Model Tuning Setup ---\")\n",
        "\n",
        "class PurgedKFold(KFold):\n",
        "    def __init__(self, n_splits=8, embargo=10):\n",
        "        super().__init__(n_splits=n_splits, shuffle=False)\n",
        "        self.embargo = embargo\n",
        "\n",
        "    def split(self, X, y=None, groups=None):\n",
        "        if not hasattr(X, 'shape') or not hasattr(X, '__len__'): X_arr = np.asarray(X)\n",
        "        else: X_arr = X\n",
        "        if len(X_arr) < self.get_n_splits(X_arr, y, groups): return\n",
        "        for train_idx, test_idx in super().split(X_arr, y, groups):\n",
        "            purged_train_idx = train_idx[train_idx < test_idx[0] - self.embargo]\n",
        "            if len(purged_train_idx) == 0: continue\n",
        "            yield purged_train_idx, test_idx\n",
        "\n",
        "def tune_model(model_cls, model_name: str, param_space, X, y, cv, gpu_params=None):\n",
        "    if gpu_params is None: gpu_params = {}\n",
        "    def objective(trial):\n",
        "        params = {}\n",
        "        for k, v_config in param_space.items():\n",
        "            if isinstance(v_config, list): params[k] = trial.suggest_categorical(k, v_config)\n",
        "            elif isinstance(v_config, tuple) and len(v_config) == 3 and v_config[2] == 'log': params[k] = trial.suggest_float(k, v_config[0], v_config[1], log=True)\n",
        "            elif isinstance(v_config, tuple) and len(v_config) == 2: params[k] = trial.suggest_float(k, v_config[0], v_config[1])\n",
        "            else: print(f\"Warning: Unsupported param_space config for {k}: {v_config} in {model_name}.\")\n",
        "\n",
        "        current_params = {**params, **gpu_params} # Merge Optuna params with fixed GPU params\n",
        "\n",
        "        if model_cls == CatBoostRegressor:\n",
        "            current_params['verbose'] = 0\n",
        "            if 'depth' in current_params and current_params['depth'] is not None: current_params['depth'] = int(current_params['depth'])\n",
        "\n",
        "        # For LGBM, n_estimators is often passed as 'n_rounds' in CV, but here it's a direct param.\n",
        "        # Max_depth = -1 is no limit for LGBM.\n",
        "        # For XGB, n_estimators is a direct param.\n",
        "\n",
        "        model = model_cls(**current_params, random_state=42)\n",
        "        try:\n",
        "            scores = cross_val_score(model, X, y, cv=cv, scoring='neg_mean_absolute_error', error_score='raise')\n",
        "        except Exception as e_cv_score:\n",
        "            print(f\"Debug: cross_val_score failed for {model_name} with params {current_params}. Error: {e_cv_score}\")\n",
        "            return np.nan\n",
        "        if len(scores) == 0 or np.all(np.isnan(scores)): return np.nan\n",
        "        return -np.nanmean(scores)\n",
        "\n",
        "    study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42, multivariate=True))\n",
        "    # Reduced n_trials for faster example, increase for real tuning\n",
        "    study.optimize(objective, n_trials=30, timeout=600, show_progress_bar=True)\n",
        "\n",
        "    best_params_from_study = {**study.best_params, **gpu_params} # Combine best Optuna params with fixed GPU params\n",
        "    if model_cls == CatBoostRegressor:\n",
        "        best_params_from_study['verbose'] = 0\n",
        "        if 'depth' in best_params_from_study and best_params_from_study['depth'] is not None: best_params_from_study['depth'] = int(best_params_from_study['depth'])\n",
        "\n",
        "    return model_cls(**best_params_from_study, random_state=42)\n",
        "\n",
        "\n",
        "def fit_models_on_gpu(X, y, asset_name_logging=\"\"):\n",
        "    cv = PurgedKFold(n_splits=8, embargo=10) # n_splits=5 for smaller datasets if needed\n",
        "    num_potential_splits = 0\n",
        "    if X.shape[0] >= cv.get_n_splits():\n",
        "        num_potential_splits = sum(1 for _ in cv.split(X, y))\n",
        "\n",
        "    # GPU parameters\n",
        "    xgb_gpu_params = {'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
        "    lgbm_gpu_params = {'device': 'gpu', 'gpu_platform_id': 0, 'gpu_device_id': 0} # Check platform/device IDs if multiple GPUs/platforms\n",
        "    catboost_gpu_params = {'task_type': 'GPU', 'devices': '0'}\n",
        "\n",
        "    model_configs = {\n",
        "        \"LGBM_GPU\": (lgb.LGBMRegressor, {\n",
        "            \"num_leaves\": [31, 63, 127], \"learning_rate\": (1e-3, 0.2, 'log'),\n",
        "            \"max_depth\": [-1, 6, 12], \"n_estimators\": [100, 200, 400] # Added n_estimators\n",
        "        }, lgbm_gpu_params),\n",
        "        \"CatBoost_GPU\": (CatBoostRegressor, {\n",
        "            \"depth\": [4, 6, 8], \"learning_rate\": (1e-3, 0.2, 'log'),\n",
        "            \"iterations\": [100, 200, 400] # iterations is n_estimators for CatBoost\n",
        "        }, catboost_gpu_params),\n",
        "        \"XGB_GPU\": (XGBRegressor, {\n",
        "            \"n_estimators\": [100, 200, 400], \"learning_rate\": (0.01, 0.2, 'log'),\n",
        "            \"max_depth\": [3, 6, 9]\n",
        "        }, xgb_gpu_params),\n",
        "        \"RF_CPU\": (RandomForestRegressor, { # RandomForest remains on CPU\n",
        "            \"n_estimators\": [100, 200, 300], \"max_depth\": [8, 16, None]\n",
        "        }, {}), # Empty dict for gpu_params for RF\n",
        "    }\n",
        "\n",
        "    if num_potential_splits == 0:\n",
        "        print(f\"Warning [{asset_name_logging}]: PurgedKFold yields no valid splits. Fitting models with default parameters (GPU where applicable).\")\n",
        "        fitted_models_fallback = {}\n",
        "        for name, (model_cls, _, current_gpu_params) in model_configs.items():\n",
        "            print(f\"  Fitting fallback {name} for asset {asset_name_logging}...\")\n",
        "            try:\n",
        "                params_with_gpu = current_gpu_params.copy()\n",
        "                if model_cls == CatBoostRegressor: params_with_gpu['verbose'] = 0\n",
        "                # Add default n_estimators if not in gpu_params\n",
        "                if model_cls in [XGBRegressor, lgb.LGBMRegressor, RandomForestRegressor] and 'n_estimators' not in params_with_gpu:\n",
        "                    if 'n_estimators' in model_configs[name][1]: # Check if defined in space\n",
        "                         params_with_gpu['n_estimators'] = model_configs[name][1]['n_estimators'][0] # take first option\n",
        "                    else: params_with_gpu['n_estimators'] = 100 # a general default\n",
        "                if model_cls == CatBoostRegressor and 'iterations' not in params_with_gpu:\n",
        "                     if 'iterations' in model_configs[name][1]:\n",
        "                         params_with_gpu['iterations'] = model_configs[name][1]['iterations'][0]\n",
        "                     else: params_with_gpu['iterations'] = 100\n",
        "\n",
        "\n",
        "                model_instance = model_cls(**params_with_gpu, random_state=42)\n",
        "                model_instance.fit(X, y)\n",
        "                fitted_models_fallback[name] = model_instance\n",
        "            except Exception as e_fit_fb:\n",
        "                print(f\"    ✗ Error fitting fallback {name} for asset {asset_name_logging}: {e_fit_fb}\")\n",
        "                traceback.print_exc()\n",
        "        return fitted_models_fallback\n",
        "\n",
        "    fitted_models = {}\n",
        "    for name, (model_cls, param_space, current_gpu_params) in model_configs.items():\n",
        "        print(f\"  Tuning {name} for asset {asset_name_logging}...\")\n",
        "        try:\n",
        "            tuned_model_instance = tune_model(model_cls, name, param_space, X, y, cv, gpu_params=current_gpu_params)\n",
        "            # tune_model returns an unfitted model with best_params (including GPU params)\n",
        "            print(f\"    Final fitting for {name} on asset {asset_name_logging} with params: {tuned_model_instance.get_params()}\")\n",
        "            tuned_model_instance.fit(X, y) # Fit on the full dataset\n",
        "            fitted_models[name] = tuned_model_instance\n",
        "            print(f\"    ✓ Tuned and fitted {name} for asset {asset_name_logging}.\")\n",
        "        except Exception as e_tune_fit:\n",
        "            print(f\"    ✗ Error during tuning or final fit for {name} on asset {asset_name_logging}: {type(e_tune_fit).__name__} - {e_tune_fit}\")\n",
        "            traceback.print_exc()\n",
        "            print(f\"    Attempting to fit {name} with default parameters (and GPU where applicable) for asset {asset_name_logging}.\")\n",
        "            try:\n",
        "                default_params_with_gpu = current_gpu_params.copy()\n",
        "                if model_cls == CatBoostRegressor: default_params_with_gpu['verbose'] = 0\n",
        "                # Add default n_estimators if not in gpu_params\n",
        "                if model_cls in [XGBRegressor, lgb.LGBMRegressor, RandomForestRegressor] and 'n_estimators' not in default_params_with_gpu:\n",
        "                    default_params_with_gpu['n_estimators'] = 100 # A general default\n",
        "                if model_cls == CatBoostRegressor and 'iterations' not in default_params_with_gpu:\n",
        "                    default_params_with_gpu['iterations'] = 100\n",
        "\n",
        "                default_model = model_cls(**default_params_with_gpu, random_state=42)\n",
        "                default_model.fit(X, y)\n",
        "                fitted_models[name] = default_model\n",
        "                print(f\"    ✓ Fitted {name} with default parameters for asset {asset_name_logging}.\")\n",
        "            except Exception as e_def_fit:\n",
        "                print(f\"      ✗ Failed default fit {name} for {asset_name_logging}: {e_def_fit}\")\n",
        "                traceback.print_exc()\n",
        "    return fitted_models\n"
      ],
      "metadata": {
        "id": "7T4sDnqlUchU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training per asset"
      ],
      "metadata": {
        "id": "re1WrRFsVETL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ──────────────── Train per Asset (Using GPU function) ────────────────\n",
        "print(\"\\n--- Training models per asset (GPU where applicable) ---\")\n",
        "asset_models = {}\n",
        "for a in datasets.keys():\n",
        "    print(f\"\\n--- Processing asset for training: {a} ---\")\n",
        "    try:\n",
        "        if a not in datasets or not isinstance(datasets[a], tuple) or len(datasets[a]) != 2:\n",
        "            print(f\"  ✗ {a} failed: Dataset not found correctly in 'datasets' dict. Skipping.\")\n",
        "            asset_models[a] = {}\n",
        "            continue\n",
        "        X_current, y_current = datasets[a]\n",
        "        if not isinstance(X_current, np.ndarray) or X_current.ndim != 2 or X_current.shape[0] == 0:\n",
        "            print(f\"  ✗ {a} failed: X data is empty or invalid. Shape: {getattr(X_current, 'shape', 'N/A')}. Skipping.\")\n",
        "            asset_models[a] = {}\n",
        "            continue\n",
        "        if not isinstance(y_current, np.ndarray) or y_current.ndim != 1 or y_current.shape[0] == 0:\n",
        "            print(f\"  ✗ {a} failed: y data is empty or invalid. Shape: {getattr(y_current, 'shape', 'N/A')}. Skipping.\")\n",
        "            asset_models[a] = {}\n",
        "            continue\n",
        "\n",
        "        print(f\"Fitting models for {a} … (X shape: {X_current.shape}, y shape: {y_current.shape})\")\n",
        "        asset_models[a] = fit_models_on_gpu(X_current, y_current, asset_name_logging=a) # Call GPU version\n",
        "\n",
        "        if asset_models.get(a): print(f\"  ✓ {a} processing done. Models: {list(asset_models[a].keys())}\")\n",
        "        else: print(f\"  ✗ {a} processing done, but no models were successfully fitted.\"); asset_models[a] = {}\n",
        "    except Exception as e_asset_loop:\n",
        "        print(f\"  ✗ {a} failed in main asset loop → {type(e_asset_loop).__name__}: {e_asset_loop}\"); traceback.print_exc(); asset_models[a] = {}\n",
        "\n",
        "print(\"\\n=== Trained Models Overview: ===\")\n",
        "if not asset_models: print(\"No assets processed or no models trained.\")\n",
        "else:\n",
        "    all_empty = True\n",
        "    for asset_k, models_v in asset_models.items():\n",
        "        if models_v and isinstance(models_v, dict) and models_v.keys(): print(f\"  Asset {asset_k}: Models - {list(models_v.keys())}\"); all_empty=False\n",
        "        else: print(f\"  Asset {asset_k}: No models successfully fitted.\")\n",
        "    if all_empty: print(\"No models were successfully trained for any asset.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "0eQuLDfVUceQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backtesting"
      ],
      "metadata": {
        "id": "8IEIkrzdVHvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ───────────────────────────────── Backtesting Section ─────────────────────────────────\n",
        "print(\"\\n--- Preparing for Backtesting ---\")\n",
        "try: from IPython.display import display\n",
        "except ImportError: display = print\n",
        "\n",
        "def prepare_live_df(asset: str) -> pd.DataFrame:\n",
        "    print(f\"\\nPreparing live dataframe for asset: {asset}\")\n",
        "    if asset not in raw_bt or raw_bt[asset].is_empty():\n",
        "        print(f\"Critical error: Raw backtest data for asset {asset} is missing or empty.\"); return pd.DataFrame()\n",
        "\n",
        "    feats_pl = engineer_features(raw_bt[asset], asset)\n",
        "    if feats_pl.is_empty():\n",
        "        print(f\"Warning: Feature engineering for backtest data of asset {asset} resulted in an empty DataFrame. Backtest may fail or be meaningless.\")\n",
        "        # Depending on strategy, might want to return empty df or df with only OHLCV if that's handled\n",
        "        # For now, let's return an empty df to signal issue clearly to backtester\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    feats_pd = feats_pl.to_pandas().reset_index(drop=True)\n",
        "    live_pd_full = raw_bt[asset].to_pandas()\n",
        "\n",
        "    if live_pd_full.empty: print(f\"Critical Error: Raw backtest data for {asset} empty.\"); return pd.DataFrame()\n",
        "\n",
        "    # Align based on potentially shorter feats_pd\n",
        "    if not feats_pd.empty: live_pd_aligned = live_pd_full.iloc[-len(feats_pd):].copy().reset_index(drop=True)\n",
        "    else: # This case should be rare if feats_pl check above is handled\n",
        "        print(f\"Warning: Features DataFrame is empty for {asset} during alignment. Using full live data for OHLCV, but features will be missing.\");\n",
        "        live_pd_aligned = live_pd_full.copy().reset_index(drop=True)\n",
        "\n",
        "    pref = PREFIX[asset]\n",
        "    rename_map = {}\n",
        "    for col_std, col_orig_pattern in [(\"Open\",f\"{pref}_open\"), (\"High\",f\"{pref}_high\"), (\"Low\",f\"{pref}_low\"), (\"Close\",f\"{pref}_close\"), (\"Volume\",f\"{pref}_volume\")]:\n",
        "        if col_orig_pattern in live_pd_aligned.columns: rename_map[col_orig_pattern] = col_std\n",
        "        elif col_std.lower() in live_pd_aligned.columns: rename_map[col_std.lower()] = col_std # Fallback to generic lower\n",
        "\n",
        "    live_pd_renamed = live_pd_aligned.rename(columns=rename_map)\n",
        "    final_df = live_pd_renamed.copy()\n",
        "\n",
        "    if not feats_pd.empty: # Add features if they exist\n",
        "        for col in feats_pd.columns:\n",
        "            if col in final_df.columns and col not in ['Open','High','Low','Close','Volume']: # Avoid overwriting standard OHLCV if a feature has same name\n",
        "                print(f\"Warning: Feature col '{col}' for {asset} clashes. Overwriting.\")\n",
        "            final_df[col] = feats_pd[col].values # Assign features\n",
        "\n",
        "    if 'ts' in final_df.columns:\n",
        "        final_df['timestamp_dt'] = pd.to_datetime(final_df['ts'])\n",
        "        final_df = final_df.set_index('timestamp_dt', drop=True)\n",
        "    else: print(f\"Warning: 'ts' col not for DatetimeIndex in {asset} backtest data.\")\n",
        "\n",
        "    required_bt_cols = [\"Open\", \"High\", \"Low\", \"Close\"]\n",
        "    missing_bt_cols = [col for col in required_bt_cols if col not in final_df.columns]\n",
        "    if missing_bt_cols: raise ValueError(f\"Asset {asset}: Missing OHLC for Backtesting.py: {missing_bt_cols}. Cols: {final_df.columns.tolist()}\")\n",
        "\n",
        "    # Ensure OHLC are numeric and not all NaN\n",
        "    for col in required_bt_cols:\n",
        "        final_df[col] = pd.to_numeric(final_df[col], errors='coerce')\n",
        "    if final_df[required_bt_cols].isnull().all().all(): # If ALL OHLC data is NaN\n",
        "        print(f\"Critical: All OHLC data for {asset} is NaN. Returning empty DataFrame.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    print(f\"Prepared live df for {asset}. Shape: {final_df.shape}, Index: {type(final_df.index)}\")\n",
        "    return final_df\n",
        "\n",
        "def build_meta_strategy(asset):\n",
        "    if not asset_models.get(asset) or not isinstance(asset_models[asset], dict) or not asset_models[asset].keys():\n",
        "        print(f\"Warning: No models for {asset}. Cannot build strategy.\"); return None\n",
        "\n",
        "    current_asset_models_dict = asset_models[asset]\n",
        "    # Filter out any models that might be None if fitting failed for them\n",
        "    valid_models = {name: model for name, model in current_asset_models_dict.items() if model is not None}\n",
        "    if not valid_models:\n",
        "        print(f\"Warning: No valid (non-None) models found for asset {asset} after filtering. Cannot build strategy.\"); return None\n",
        "\n",
        "    weights = {k: 1/len(valid_models) for k in valid_models}\n",
        "\n",
        "    class MetaVoteStrategy(Strategy):\n",
        "        # Closure variables from outer scope\n",
        "        _models_to_use_cl = valid_models\n",
        "        _model_weights_cl = weights\n",
        "        _trade_threshold_cl = 0.00 # Example: 0.00 for any signal, adjust as needed (e.g. 0.001 for 0.1% predicted return)\n",
        "        _feature_names_list_cl = None\n",
        "\n",
        "        def init(self):\n",
        "            self.models_to_use = self._models_to_use_cl\n",
        "            self.model_weights = self._model_weights_cl\n",
        "            self.trade_threshold = self._trade_threshold_cl\n",
        "\n",
        "            # One-time setup for feature names\n",
        "            if MetaVoteStrategy._feature_names_list_cl is None and self.models_to_use:\n",
        "                # Try to get feature names from the first model\n",
        "                first_model_name = list(self.models_to_use.keys())[0]\n",
        "                first_model = self.models_to_use[first_model_name]\n",
        "\n",
        "                if hasattr(first_model, 'feature_name_'): # LightGBM\n",
        "                    MetaVoteStrategy._feature_names_list_cl = first_model.feature_name_()\n",
        "                elif hasattr(first_model, 'feature_names_in_'): # Scikit-learn, CatBoost (often)\n",
        "                    MetaVoteStrategy._feature_names_list_cl = first_model.feature_names_in_\n",
        "                elif hasattr(first_model, 'get_booster') and hasattr(first_model.get_booster(), 'feature_names'): # XGBoost\n",
        "                     MetaVoteStrategy._feature_names_list_cl = first_model.get_booster().feature_names\n",
        "\n",
        "                if MetaVoteStrategy._feature_names_list_cl is None: # Fallback\n",
        "                    ohlcv_std = {'Open', 'High', 'Low', 'Close', 'Volume', 'ts', 'timestamp_dt'}\n",
        "                    # Assuming self.data.df is available at init, might not be fully populated.\n",
        "                    # This might be better done in the first call to _pred if df structure is not fixed at init.\n",
        "                    # For now, this is a best guess.\n",
        "                    if self.data and hasattr(self.data, 'df') and not self.data.df.empty:\n",
        "                         MetaVoteStrategy._feature_names_list_cl = [col for col in self.data.df.columns if col not in ohlcv_std]\n",
        "                    else: # Cannot determine feature names yet\n",
        "                        print(\"Warning: Could not determine feature names at init in MetaVoteStrategy.\")\n",
        "            self.feature_names_list_ = MetaVoteStrategy._feature_names_list_cl\n",
        "\n",
        "\n",
        "        def _pred(self):\n",
        "            if not self.feature_names_list_:\n",
        "                # Attempt to set up feature names if not done in init (e.g. self.data.df wasn't ready)\n",
        "                if MetaVoteStrategy._feature_names_list_cl is None and self.models_to_use:\n",
        "                    first_model_name = list(self.models_to_use.keys())[0]; first_model = self.models_to_use[first_model_name]\n",
        "                    if hasattr(first_model, 'feature_name_'): MetaVoteStrategy._feature_names_list_cl = first_model.feature_name_()\n",
        "                    elif hasattr(first_model, 'feature_names_in_'): MetaVoteStrategy._feature_names_list_cl = first_model.feature_names_in_\n",
        "                    elif hasattr(first_model, 'get_booster') and hasattr(first_model.get_booster(), 'feature_names'): MetaVoteStrategy._feature_names_list_cl = first_model.get_booster().feature_names\n",
        "                    if MetaVoteStrategy._feature_names_list_cl is None:\n",
        "                        ohlcv_std = {'Open', 'High', 'Low', 'Close', 'Volume', 'ts', 'timestamp_dt'}\n",
        "                        MetaVoteStrategy._feature_names_list_cl = [col for col in self.data.df.columns if col not in ohlcv_std]\n",
        "                self.feature_names_list_ = MetaVoteStrategy._feature_names_list_cl\n",
        "                if not self.feature_names_list_:\n",
        "                    print(\"CRITICAL Error: Still cannot determine feature names for prediction in MetaVoteStrategy _pred.\")\n",
        "                    return 0\n",
        "\n",
        "            try:\n",
        "                current_feature_values = [self.data.df[fn].iloc[-1] for fn in self.feature_names_list_]\n",
        "            except KeyError as e: print(f\"KeyError in _pred for feature {e}. Available: {self.data.df.columns.tolist()}\"); return 0\n",
        "            except IndexError: print(\"IndexError in _pred, likely empty data slice.\"); return 0\n",
        "\n",
        "            if np.isnan(current_feature_values).any(): return 0\n",
        "\n",
        "            feature_array = np.array(current_feature_values).reshape(1, -1)\n",
        "            preds = {}\n",
        "            for n, m in self.models_to_use.items():\n",
        "                try: preds[n] = m.predict(feature_array)[0]\n",
        "                except Exception as e_p: print(f\"Err predicting {n}: {e_p}. Assuming no change.\"); preds[n] = self.data.Close[-1]\n",
        "\n",
        "            last_price = self.data.Close[-1]\n",
        "            if last_price == 0 or np.isnan(last_price): return 0\n",
        "\n",
        "            rets = {n: (p - last_price) / last_price if not np.isnan(p) else 0 for n, p in preds.items()}\n",
        "            return sum(self.model_weights[n] * rets[n] for n in rets)\n",
        "\n",
        "        def next(self):\n",
        "            if not self.models_to_use: return\n",
        "            # Heuristic buffer for TA stability in features.\n",
        "            # If features are precomputed and NaNs handled, this might be less critical.\n",
        "            if len(self.data.Close) < max(60, lags_from_config if 'lags_from_config' in globals() else 5) + 5 : # Ensure enough data for largest window + lags\n",
        "                return\n",
        "            signal = self._pred()\n",
        "            if np.isnan(signal): signal = 0 # Handle NaN signal as neutral\n",
        "\n",
        "            if signal > self.trade_threshold and not self.position.is_long:\n",
        "                self.position.close(); self.buy()\n",
        "            elif signal < -self.trade_threshold and not self.position.is_short:\n",
        "                self.position.close(); self.sell()\n",
        "    return MetaVoteStrategy\n"
      ],
      "metadata": {
        "id": "YjTJqCHJUcZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backtesting per asset"
      ],
      "metadata": {
        "id": "gN4UwGR6VGfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---------- run back‑test per asset ----------\n",
        "print(\"\\n--- Running Backtests ---\")\n",
        "stats = {}\n",
        "equity_curves = {}\n",
        "# Use keys from asset_models as these are the assets for which training was attempted\n",
        "# This ensures we only try to backtest assets that might have models.\n",
        "assets_to_backtest = list(asset_models.keys())\n",
        "\n",
        "for asset_key_bt in assets_to_backtest:\n",
        "    print(f\"\\n--- Running Backtest for: {asset_key_bt} ---\")\n",
        "    # Check again if models actually exist and are valid for this asset\n",
        "    if not asset_models.get(asset_key_bt) or not any(asset_models[asset_key_bt].values()):\n",
        "        print(f\"Skipping backtest for {asset_key_bt}: No valid models were trained or found.\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        live_df_prepared = prepare_live_df(asset_key_bt)\n",
        "        if live_df_prepared.empty or not isinstance(live_df_prepared.index, pd.DatetimeIndex):\n",
        "            print(f\"Skipping backtest for {asset_key_bt}: Live DataFrame unusable.\"); continue\n",
        "        if live_df_prepared[['Open', 'High', 'Low', 'Close']].isnull().values.any():\n",
        "            print(f\"Skipping backtest for {asset_key_bt}: Live DataFrame OHLC NaNs.\"); continue\n",
        "\n",
        "        StratBuilder = build_meta_strategy(asset_key_bt)\n",
        "        if StratBuilder is None: print(f\"Skipping {asset_key_bt}: Strategy not built.\"); continue\n",
        "\n",
        "        bt_instance = Backtest(live_df_prepared, StratBuilder, cash=1_000_000, commission=0.0015, exclusive_orders=False)\n",
        "        s_result = bt_instance.run()\n",
        "        stats[asset_key_bt] = s_result\n",
        "        if '_equity_curve' in s_result and not s_result['_equity_curve'].empty:\n",
        "             equity_curves[asset_key_bt] = s_result['_equity_curve']['Equity']\n",
        "        # Optional plot: bt_instance.plot(filename=f\"backtest_{asset_key_bt}.html\", open_browser=False)\n",
        "    except Exception as e_bt_general:\n",
        "        print(f\"Error during backtest for {asset_key_bt}: {type(e_bt_general).__name__} - {e_bt_general}\"); traceback.print_exc()\n",
        "\n",
        "# ---------- summary table & equity curves ----------\n",
        "if stats:\n",
        "    summary_data = {}\n",
        "    for a, s_val in stats.items():\n",
        "        if hasattr(s_val, 'get'): # Handles Series/dict from Backtesting.py\n",
        "            summary_data[a] = {\n",
        "                \"APR\": s_val.get('Return (Ann.) [%]', np.nan),\n",
        "                \"Sharpe\": s_val.get('Sharpe Ratio', np.nan),\n",
        "                \"MaxDD\": s_val.get('Max. Drawdown [%]', np.nan),\n",
        "                \"WinRate\": s_val.get('Win Rate [%]', np.nan) / 100 if pd.notna(s_val.get('Win Rate [%]')) else np.nan,\n",
        "                \"Trades\": s_val.get('# Trades', 0),\n",
        "                \"Exposure\": s_val.get('Exposure Time [%]', np.nan) / 100 if pd.notna(s_val.get('Exposure Time [%]')) else np.nan\n",
        "            }\n",
        "    summary = pd.DataFrame(summary_data).T.round(3)\n",
        "    print(\"\\n=== Hold‑out performance ===\")\n",
        "    if not summary.empty: display(summary)\n",
        "    else: print(\"No summary data to display.\")\n",
        "else: print(\"\\nNo backtesting statistics were generated.\")\n",
        "\n",
        "if equity_curves:\n",
        "    plt.figure(figsize=(12,7))\n",
        "    for asset_label_eq, curve_data in equity_curves.items():\n",
        "        if not curve_data.empty: plt.plot(curve_data.index, curve_data, label=asset_label_eq.upper())\n",
        "    plt.title(\"Equity Curves ‑ Hold‑out Period\"); plt.xlabel(\"Date\"); plt.ylabel(\"Equity ($)\"); plt.legend(); plt.grid(True);\n",
        "    # Save plot instead of showing if in non-interactive environment\n",
        "    # plt.savefig(\"equity_curves.png\")\n",
        "    plt.show()\n",
        "else: print(\"No equity curves to plot.\")\n",
        "\n",
        "print(\"\\n--- Script End ---\")\n",
        "\n",
        "# Optional: Clean up the global temporary directory\n",
        "# import shutil\n",
        "# if '_temp_dir_for_dummy_files' in globals() and os.path.exists(_temp_dir_for_dummy_files) and _temp_dir_for_dummy_files != \".\":\n",
        "#     try:\n",
        "#         print(f\"INFO: Removing temporary directory: {_temp_dir_for_dummy_files}\")\n",
        "#         shutil.rmtree(_temp_dir_for_dummy_files)\n",
        "#     except Exception as e_shutil:\n",
        "#         print(f\"Warning: Could not remove temporary directory {_temp_dir_for_dummy_files}: {e_shutil}\")\n"
      ],
      "metadata": {
        "id": "_WtO6zPlUcWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FmJyzkyaUcUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2UDj9CEEUcRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GPyO5_JtUcO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5WTqVCASUcFB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}