{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35118933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(89125) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (2.1.3)\n",
      "Requirement already satisfied: pandas in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (1.6.1)\n",
      "Requirement already satisfied: xgboost in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (3.0.1)\n",
      "Requirement already satisfied: tensorflow in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (2.19.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (3.9.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: namex in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
      "Requirement already satisfied: optree in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas scikit-learn xgboost tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3d870c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cec9288",
   "metadata": {},
   "source": [
    "For ada solana and eth: Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1c4cdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ada long:    eth_open  eth_high  eth_low  eth_close  eth_volume  ada_open  ada_high  \\\n",
      "0    377.74    378.24   377.43     377.94  1066.72997   0.13618   0.13639   \n",
      "1    377.92    378.17   377.88     377.95   399.58109   0.13618   0.13629   \n",
      "2    377.95    378.71   377.88     378.65   686.35004   0.13617   0.13629   \n",
      "3    378.65    379.77   378.65     379.75  1301.39604   0.13626   0.13668   \n",
      "4    379.77    379.93   379.69     379.85   319.49589   0.13671   0.13684   \n",
      "\n",
      "   ada_low  ada_close  ada_volume  ...  solana_close_lag3  \\\n",
      "0  0.13606    0.13622    397998.4  ...             3.2985   \n",
      "1  0.13609    0.13620    182810.7  ...             3.2815   \n",
      "2  0.13595    0.13622    386508.3  ...             3.2690   \n",
      "3  0.13626    0.13666    198676.4  ...             3.2676   \n",
      "4  0.13670    0.13672    128888.1  ...             3.2538   \n",
      "\n",
      "                    timestamp   ada_log     ada_r  ada_r_next  ada_r_winsor  \\\n",
      "0  2020-08-12T00:03:00.000000 -1.993484 -0.000440   -0.000147     -0.000440   \n",
      "1  2020-08-12T00:04:00.000000 -1.993631 -0.000147    0.000147     -0.000147   \n",
      "2  2020-08-12T00:05:00.000000 -1.993484  0.000147    0.003225      0.000147   \n",
      "3  2020-08-12T00:06:00.000000 -1.990259  0.003225    0.000439      0.003225   \n",
      "4  2020-08-12T00:07:00.000000 -1.989820  0.000439   -0.000439      0.000439   \n",
      "\n",
      "   ada_r_ma1h  ada_r_vol1h  ada_r_ma1d  ada_r_vol1d  \n",
      "0   -0.000367     0.000773   -0.000367     0.000773  \n",
      "1   -0.000312     0.000640   -0.000312     0.000640  \n",
      "2   -0.000220     0.000591   -0.000220     0.000591  \n",
      "3    0.000354     0.001503    0.000354     0.001503  \n",
      "4    0.000366     0.001372    0.000366     0.001372  \n",
      "\n",
      "[5 rows x 34 columns]\n",
      "Solana long:    eth_open  eth_high  eth_low  eth_close  eth_volume  ada_open  ada_high  \\\n",
      "0    377.74    378.24   377.43     377.94  1066.72997   0.13618   0.13639   \n",
      "1    377.92    378.17   377.88     377.95   399.58109   0.13618   0.13629   \n",
      "2    377.95    378.71   377.88     378.65   686.35004   0.13617   0.13629   \n",
      "3    378.65    379.77   378.65     379.75  1301.39604   0.13626   0.13668   \n",
      "4    379.77    379.93   379.69     379.85   319.49589   0.13671   0.13684   \n",
      "\n",
      "   ada_low  ada_close  ada_volume  ...  solana_close_lag3  \\\n",
      "0  0.13606    0.13622    397998.4  ...             3.2985   \n",
      "1  0.13609    0.13620    182810.7  ...             3.2815   \n",
      "2  0.13595    0.13622    386508.3  ...             3.2690   \n",
      "3  0.13626    0.13666    198676.4  ...             3.2676   \n",
      "4  0.13670    0.13672    128888.1  ...             3.2538   \n",
      "\n",
      "                    timestamp  solana_log  solana_r  solana_r_next  \\\n",
      "0  2020-08-12T00:03:00.000000    1.184056 -0.000428      -0.004232   \n",
      "1  2020-08-12T00:04:00.000000    1.179824 -0.004232      -0.003263   \n",
      "2  2020-08-12T00:05:00.000000    1.176560 -0.003263       0.003263   \n",
      "3  2020-08-12T00:06:00.000000    1.179824  0.003263      -0.008488   \n",
      "4  2020-08-12T00:07:00.000000    1.171336 -0.008488       0.001363   \n",
      "\n",
      "   solana_r_winsor  solana_r_ma1h  solana_r_vol1h  solana_r_ma1d  \\\n",
      "0        -0.000428      -0.003137        0.002441      -0.003137   \n",
      "1        -0.004232      -0.003411        0.002067      -0.003411   \n",
      "2        -0.003263      -0.003381        0.001791      -0.003381   \n",
      "3         0.003263      -0.002274        0.003150      -0.002274   \n",
      "4        -0.005746      -0.002770        0.003161      -0.002770   \n",
      "\n",
      "   solana_r_vol1d  \n",
      "0        0.002441  \n",
      "1        0.002067  \n",
      "2        0.001791  \n",
      "3        0.003150  \n",
      "4        0.003161  \n",
      "\n",
      "[5 rows x 34 columns]\n",
      "ETH long:    eth_open  eth_high  eth_low  eth_close  eth_volume  ada_open  ada_high  \\\n",
      "0    377.74    378.24   377.43     377.94  1066.72997   0.13618   0.13639   \n",
      "1    377.92    378.17   377.88     377.95   399.58109   0.13618   0.13629   \n",
      "2    377.95    378.71   377.88     378.65   686.35004   0.13617   0.13629   \n",
      "3    378.65    379.77   378.65     379.75  1301.39604   0.13626   0.13668   \n",
      "4    379.77    379.93   379.69     379.85   319.49589   0.13671   0.13684   \n",
      "\n",
      "   ada_low  ada_close  ada_volume  ...  solana_close_lag3  \\\n",
      "0  0.13606    0.13622    397998.4  ...             3.2985   \n",
      "1  0.13609    0.13620    182810.7  ...             3.2815   \n",
      "2  0.13595    0.13622    386508.3  ...             3.2690   \n",
      "3  0.13626    0.13666    198676.4  ...             3.2676   \n",
      "4  0.13670    0.13672    128888.1  ...             3.2538   \n",
      "\n",
      "                    timestamp   eth_log     eth_r  eth_r_next  eth_r_winsor  \\\n",
      "0  2020-08-12T00:03:00.000000  5.934735  0.000582    0.000026      0.000582   \n",
      "1  2020-08-12T00:04:00.000000  5.934762  0.000026    0.001850      0.000026   \n",
      "2  2020-08-12T00:05:00.000000  5.936612  0.001850    0.002901      0.001850   \n",
      "3  2020-08-12T00:06:00.000000  5.939513  0.002901    0.000263      0.002901   \n",
      "4  2020-08-12T00:07:00.000000  5.939776  0.000263    0.000684      0.000263   \n",
      "\n",
      "   eth_r_ma1h  eth_r_vol1h  eth_r_ma1d  eth_r_vol1d  \n",
      "0   -0.000599     0.001097   -0.000599     0.001097  \n",
      "1   -0.000443     0.000949   -0.000443     0.000949  \n",
      "2    0.000016     0.001314    0.000016     0.001314  \n",
      "3    0.000497     0.001664    0.000497     0.001664  \n",
      "4    0.000463     0.001522    0.000463     0.001522  \n",
      "\n",
      "[5 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "data_ada_long = pd.read_csv('Users/jadenfix/downloads/processed_data_ada_long.csv')\n",
    "print(\"Ada long:\",data_ada_long.head())\n",
    "\n",
    "data_solana_long = pd.read_csv('Users/jadenfix/downloads/processed_data_solana_long.csv')\n",
    "print(\"Solana long:\",data_solana_long.head())\n",
    "\n",
    "data_eth_long = pd.read_csv('Users/jadenfix/downloads/processed_data_eth_long.csv')\n",
    "print(\"ETH long:\",data_eth_long.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7776b677",
   "metadata": {},
   "source": [
    "For short: ada btc eth and solana\n",
    "/Users/jadenfix/Downloads/processed_data_solana.csv /Users/jadenfix/Downloads/processed_data_ada.csv /Users/jadenfix/Downloads/processed_data_eth.csv /Users/jadenfix/Downloads/processed_data_btc.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dac83b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ada short:    btc_open  btc_high   btc_low  btc_close  btc_volume  eth_open  eth_high  \\\n",
      "0  29050.29  29050.29  29050.29   29050.29         0.0   1829.91   1830.50   \n",
      "1  29050.29  29050.29  29050.29   29050.29         0.0   1830.49   1830.68   \n",
      "2  29050.29  29050.29  29050.29   29050.29         0.0   1830.67   1830.76   \n",
      "3  29050.29  29050.29  29050.29   29050.29         0.0   1830.76   1830.93   \n",
      "4  29050.29  29050.29  29050.29   29050.29         0.0   1830.92   1831.17   \n",
      "\n",
      "   eth_low  eth_close  eth_volume  ...  solana_close_lag3  \\\n",
      "0  1829.53    1830.49    362.8306  ...              22.79   \n",
      "1  1830.49    1830.68     29.8070  ...              22.77   \n",
      "2  1830.67    1830.75     28.3587  ...              22.77   \n",
      "3  1830.75    1830.92     14.4738  ...              22.77   \n",
      "4  1830.92    1831.17     82.1231  ...              22.78   \n",
      "\n",
      "                    timestamp   ada_log     ada_r  ada_r_next  ada_r_winsor  \\\n",
      "0  2023-08-05T00:03:00.000000 -1.225878  0.000000    0.000341      0.000000   \n",
      "1  2023-08-05T00:04:00.000000 -1.225537  0.000341   -0.000681      0.000341   \n",
      "2  2023-08-05T00:05:00.000000 -1.226218 -0.000681    0.000341     -0.000681   \n",
      "3  2023-08-05T00:06:00.000000 -1.225878  0.000341    0.000681      0.000341   \n",
      "4  2023-08-05T00:07:00.000000 -1.225196  0.000681   -0.000681      0.000681   \n",
      "\n",
      "   ada_r_ma1h  ada_r_vol1h  ada_r_ma1d  ada_r_vol1d  \n",
      "0    0.000114     0.000197    0.000114     0.000197  \n",
      "1    0.000170     0.000197    0.000170     0.000197  \n",
      "2    0.000000     0.000417    0.000000     0.000417  \n",
      "3    0.000057     0.000398    0.000057     0.000398  \n",
      "4    0.000146     0.000433    0.000146     0.000433  \n",
      "\n",
      "[5 rows x 42 columns]\n",
      "Solana short:    btc_open  btc_high   btc_low  btc_close  btc_volume  eth_open  eth_high  \\\n",
      "0  29050.29  29050.29  29050.29   29050.29         0.0   1829.91   1830.50   \n",
      "1  29050.29  29050.29  29050.29   29050.29         0.0   1830.49   1830.68   \n",
      "2  29050.29  29050.29  29050.29   29050.29         0.0   1830.67   1830.76   \n",
      "3  29050.29  29050.29  29050.29   29050.29         0.0   1830.76   1830.93   \n",
      "4  29050.29  29050.29  29050.29   29050.29         0.0   1830.92   1831.17   \n",
      "\n",
      "   eth_low  eth_close  eth_volume  ...  solana_close_lag3  \\\n",
      "0  1829.53    1830.49    362.8306  ...              22.79   \n",
      "1  1830.49    1830.68     29.8070  ...              22.77   \n",
      "2  1830.67    1830.75     28.3587  ...              22.77   \n",
      "3  1830.75    1830.92     14.4738  ...              22.77   \n",
      "4  1830.92    1831.17     82.1231  ...              22.78   \n",
      "\n",
      "                    timestamp  solana_log  solana_r  solana_r_next  \\\n",
      "0  2023-08-05T00:03:00.000000    3.125444  0.000000       0.000439   \n",
      "1  2023-08-05T00:04:00.000000    3.125883  0.000439       0.000000   \n",
      "2  2023-08-05T00:05:00.000000    3.125883  0.000000       0.000439   \n",
      "3  2023-08-05T00:06:00.000000    3.126322  0.000439       0.000877   \n",
      "4  2023-08-05T00:07:00.000000    3.127199  0.000877       0.000438   \n",
      "\n",
      "   solana_r_winsor  solana_r_ma1h  solana_r_vol1h  solana_r_ma1d  \\\n",
      "0         0.000000      -0.000293        0.000507      -0.000293   \n",
      "1         0.000439      -0.000110        0.000552      -0.000110   \n",
      "2         0.000000      -0.000088        0.000481      -0.000088   \n",
      "3         0.000439       0.000000        0.000481       0.000000   \n",
      "4         0.000877       0.000125        0.000550       0.000125   \n",
      "\n",
      "   solana_r_vol1d  \n",
      "0        0.000507  \n",
      "1        0.000552  \n",
      "2        0.000481  \n",
      "3        0.000481  \n",
      "4        0.000550  \n",
      "\n",
      "[5 rows x 42 columns]\n",
      "ETH short:    btc_open  btc_high   btc_low  btc_close  btc_volume  eth_open  eth_high  \\\n",
      "0  29050.29  29050.29  29050.29   29050.29         0.0   1829.91   1830.50   \n",
      "1  29050.29  29050.29  29050.29   29050.29         0.0   1830.49   1830.68   \n",
      "2  29050.29  29050.29  29050.29   29050.29         0.0   1830.67   1830.76   \n",
      "3  29050.29  29050.29  29050.29   29050.29         0.0   1830.76   1830.93   \n",
      "4  29050.29  29050.29  29050.29   29050.29         0.0   1830.92   1831.17   \n",
      "\n",
      "   eth_low  eth_close  eth_volume  ...  solana_close_lag3  \\\n",
      "0  1829.53    1830.49    362.8306  ...              22.79   \n",
      "1  1830.49    1830.68     29.8070  ...              22.77   \n",
      "2  1830.67    1830.75     28.3587  ...              22.77   \n",
      "3  1830.75    1830.92     14.4738  ...              22.77   \n",
      "4  1830.92    1831.17     82.1231  ...              22.78   \n",
      "\n",
      "                    timestamp   eth_log     eth_r  eth_r_next  eth_r_winsor  \\\n",
      "0  2023-08-05T00:03:00.000000  7.512339  0.000311    0.000104      0.000311   \n",
      "1  2023-08-05T00:04:00.000000  7.512443  0.000104    0.000038      0.000104   \n",
      "2  2023-08-05T00:05:00.000000  7.512481  0.000038    0.000093      0.000038   \n",
      "3  2023-08-05T00:06:00.000000  7.512574  0.000093    0.000137      0.000093   \n",
      "4  2023-08-05T00:07:00.000000  7.512710  0.000137   -0.000027      0.000137   \n",
      "\n",
      "   eth_r_ma1h  eth_r_vol1h  eth_r_ma1d  eth_r_vol1d  \n",
      "0    0.000104     0.000180    0.000104     0.000180  \n",
      "1    0.000104     0.000147    0.000104     0.000147  \n",
      "2    0.000091     0.000130    0.000091     0.000130  \n",
      "3    0.000091     0.000117    0.000091     0.000117  \n",
      "4    0.000098     0.000108    0.000098     0.000108  \n",
      "\n",
      "[5 rows x 42 columns]\n",
      "BTC short:    btc_open  btc_high   btc_low  btc_close  btc_volume  eth_open  eth_high  \\\n",
      "0  29050.29  29050.29  29050.29   29050.29         0.0   1829.91   1830.50   \n",
      "1  29050.29  29050.29  29050.29   29050.29         0.0   1830.49   1830.68   \n",
      "2  29050.29  29050.29  29050.29   29050.29         0.0   1830.67   1830.76   \n",
      "3  29050.29  29050.29  29050.29   29050.29         0.0   1830.76   1830.93   \n",
      "4  29050.29  29050.29  29050.29   29050.29         0.0   1830.92   1831.17   \n",
      "\n",
      "   eth_low  eth_close  eth_volume  ...  solana_close_lag3  \\\n",
      "0  1829.53    1830.49    362.8306  ...              22.79   \n",
      "1  1830.49    1830.68     29.8070  ...              22.77   \n",
      "2  1830.67    1830.75     28.3587  ...              22.77   \n",
      "3  1830.75    1830.92     14.4738  ...              22.77   \n",
      "4  1830.92    1831.17     82.1231  ...              22.78   \n",
      "\n",
      "                    timestamp    btc_log  btc_r  btc_r_next  btc_r_winsor  \\\n",
      "0  2023-08-05T00:03:00.000000  10.276784    0.0         0.0           0.0   \n",
      "1  2023-08-05T00:04:00.000000  10.276784    0.0         0.0           0.0   \n",
      "2  2023-08-05T00:05:00.000000  10.276784    0.0         0.0           0.0   \n",
      "3  2023-08-05T00:06:00.000000  10.276784    0.0         0.0           0.0   \n",
      "4  2023-08-05T00:07:00.000000  10.276784    0.0         0.0           0.0   \n",
      "\n",
      "   btc_r_ma1h  btc_r_vol1h  btc_r_ma1d  btc_r_vol1d  \n",
      "0         0.0          0.0         0.0          0.0  \n",
      "1         0.0          0.0         0.0          0.0  \n",
      "2         0.0          0.0         0.0          0.0  \n",
      "3         0.0          0.0         0.0          0.0  \n",
      "4         0.0          0.0         0.0          0.0  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "data_ada_short = pd.read_csv('Users/jadenfix/downloads/processed_data_ada.csv')\n",
    "print(\"Ada short:\",data_ada_short.head())\n",
    "data_solana_short = pd.read_csv('Users/jadenfix/downloads/processed_data_solana.csv')\n",
    "print(\"Solana short:\",data_solana_short.head())\n",
    "data_eth_short = pd.read_csv('Users/jadenfix/downloads/processed_data_eth.csv')\n",
    "print(\"ETH short:\",data_eth_short.head())\n",
    "data_btc_short = pd.read_csv('Users/jadenfix/downloads/processed_data_btc.csv')\n",
    "print(\"BTC short:\",data_btc_short.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1eb84d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 964us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 726us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 453us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 343us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 292us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 403us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 298us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 448us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 458us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 494us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 505us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 562us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 482us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 284us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 421us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 316us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 571us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 300us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 383us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 410us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 429us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 350us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 306us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 323us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 355us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 357us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 346us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 291us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 317us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 315us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 293us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265us/step\n",
      "      coin          model       mae      rmse            r2  mape\n",
      "0      ada  BayesianRidge  0.000314  0.000526      0.732095   inf\n",
      "1      ada        XGBoost  0.001183  0.001599     -2.739989   inf\n",
      "2      ada       AdaBoost  0.000723  0.001211     -0.807472   inf\n",
      "3      ada            MLP  0.032039  0.048455  -7404.689256   inf\n",
      "4      ada            CNN  0.009489  0.017212   -935.336309   inf\n",
      "5      ada            RNN  0.070889  0.087756 -14401.782840   inf\n",
      "6      ada           PINN  0.041914  0.053716  -6379.258573   inf\n",
      "7      ada        Stacked  0.000258  0.000451      0.823594   inf\n",
      "8   solana  BayesianRidge  0.000823  0.001402      0.085203   inf\n",
      "9   solana        XGBoost  0.001076  0.001549     -0.073499   inf\n",
      "10  solana       AdaBoost  0.001082  0.001778     -0.731032   inf\n",
      "11  solana            MLP  0.038952  0.052468  -2031.786506   inf\n",
      "12  solana            CNN  0.026064  0.040224   -922.144412   inf\n",
      "13  solana            RNN  0.048388  0.060126  -5211.131619   inf\n",
      "14  solana           PINN  0.025214  0.034335   -926.385693   inf\n",
      "15  solana        Stacked  0.000597  0.001088      0.381056   inf\n",
      "16     eth  BayesianRidge  0.000065  0.000108      0.967003   inf\n",
      "17     eth        XGBoost  0.000530  0.000870     -1.267007   inf\n",
      "18     eth       AdaBoost  0.000401  0.000639     -0.022713   inf\n",
      "19     eth            MLP  0.067586  0.091958 -29132.873610   inf\n",
      "20     eth            CNN  0.032021  0.045348  -8033.626875   inf\n",
      "21     eth            RNN  0.069068  0.090079 -27590.104049   inf\n",
      "22     eth           PINN  0.018299  0.024816  -2263.332846   inf\n",
      "23     eth        Stacked  0.000067  0.000109      0.969568   inf\n",
      "24     btc  BayesianRidge  0.000087  0.000142      0.925084   inf\n",
      "25     btc        XGBoost  0.000401  0.000631     -0.249700   inf\n",
      "26     btc       AdaBoost  0.000354  0.000575     -0.005809   inf\n",
      "27     btc            MLP  0.061784  0.084336 -45229.280229   inf\n",
      "28     btc            CNN  0.010191  0.014663  -1544.681032   inf\n",
      "29     btc            RNN  0.029534  0.039574  -5145.440101   inf\n",
      "30     btc           PINN  0.025018  0.037879  -8470.980382   inf\n",
      "31     btc        Stacked  0.000084  0.000137      0.931765   inf\n"
     ]
    }
   ],
   "source": [
    "# crypto_time_series_forecast.py\n",
    "# ---------------------------------------\n",
    "# Time-Series Forecasting of Crypto Returns\n",
    "# Models: PINN, CNN, RNN, MLP, BayesianRidge, XGBoost, AdaBoost, Stacked Ensemble\n",
    "# Uses TimeSeriesSplit for robust time-series cross-validation on one-minute data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.ensemble import AdaBoostRegressor, StackingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Sequential, callbacks\n",
    "\n",
    "# 1) LOAD DATA\n",
    "paths = {\n",
    "    'ada':   '/Users/jadenfix/downloads/processed_data_ada.csv',\n",
    "    'solana':'/Users/jadenfix/downloads/processed_data_solana.csv',\n",
    "    'eth':   '/Users/jadenfix/downloads/processed_data_eth.csv',\n",
    "    'btc':   '/Users/jadenfix/downloads/processed_data_btc.csv'\n",
    "}\n",
    "# parse timestamp\n",
    "data = {coin: pd.read_csv(path, parse_dates=['timestamp']) for coin, path in paths.items()}\n",
    "\n",
    "# 2) UTIL FUNCTIONS\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    # Adds MAPE for percent error insight\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    return {'mae': mae, 'rmse': rmse, 'r2': r2, 'mape': mape}\n",
    "\n",
    "# Generic TS CV for sklearn-style models\n",
    "def cv_sklearn_model(model, X, y, n_splits=3):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    fold_metrics = []\n",
    "    for train_idx, test_idx in tscv.split(X):\n",
    "        X_tr, X_te = X[train_idx], X[test_idx]\n",
    "        y_tr, y_te = y[train_idx], y[test_idx]\n",
    "        model.fit(X_tr, y_tr)\n",
    "        preds = model.predict(X_te)\n",
    "        fold_metrics.append(metrics(y_te, preds))\n",
    "    return {k: np.mean([fm[k] for fm in fold_metrics]) for k in fold_metrics[0]}\n",
    "\n",
    "# Prepare features/target\n",
    "def prepare_xy(df, coin):\n",
    "    df = df.sort_values('timestamp').dropna()\n",
    "    X = df.drop(columns=[f'{coin}_r_next', 'timestamp']).values\n",
    "    y = df[f'{coin}_r_next'].values\n",
    "    return X, y\n",
    "\n",
    "# 3) SKLEARN MODEL FUNCTIONS\n",
    "\n",
    "def bayesian_ridge_cv(X, y):\n",
    "    return cv_sklearn_model(BayesianRidge(), X, y)\n",
    "\n",
    "\n",
    "def xgboost_cv(X, y):\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=50,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        objective='reg:squarederror',\n",
    "        verbosity=0\n",
    "    )\n",
    "    return cv_sklearn_model(model, X, y)\n",
    "\n",
    "\n",
    "def adaboost_cv(X, y):\n",
    "    base = DecisionTreeRegressor(max_depth=3)\n",
    "    model = AdaBoostRegressor(\n",
    "        estimator=base,\n",
    "        n_estimators=30,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "    return cv_sklearn_model(model, X, y)\n",
    "\n",
    "# 4) NEURAL-NET MODELS\n",
    "\n",
    "# MLP builder and CV\n",
    "def build_mlp(n_features):\n",
    "    return Sequential([\n",
    "        layers.Input(shape=(n_features,)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "def mlp_cv(X, y, n_splits=3, epochs=10, batch_size=256):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    fold_metrics = []\n",
    "    for tr_idx, te_idx in tscv.split(X):\n",
    "        X_tr, X_te = X[tr_idx], X[te_idx]\n",
    "        y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "        scaler = StandardScaler().fit(X_tr)\n",
    "        X_tr_s = scaler.transform(X_tr)\n",
    "        X_te_s = scaler.transform(X_te)\n",
    "        model = build_mlp(X_tr_s.shape[1])\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        model.fit(\n",
    "            X_tr_s, y_tr,\n",
    "            validation_data=(X_te_s, y_te),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[callbacks.EarlyStopping(patience=2, restore_best_weights=True)],\n",
    "            verbose=0\n",
    "        )\n",
    "        preds = model.predict(X_te_s).flatten()\n",
    "        fold_metrics.append(metrics(y_te, preds))\n",
    "        tf.keras.backend.clear_session()\n",
    "    return {k: np.mean([fm[k] for fm in fold_metrics]) for k in fold_metrics[0]}\n",
    "\n",
    "# CNN builder and CV\n",
    "def build_cnn(n_features):\n",
    "    return Sequential([\n",
    "        layers.Input(shape=(n_features, 1)),\n",
    "        layers.Conv1D(32, kernel_size=3, activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "def cnn_cv(X, y, n_splits=3, epochs=10, batch_size=256):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    fold_metrics = []\n",
    "    for tr_idx, te_idx in tscv.split(X):\n",
    "        X_tr, X_te = X[tr_idx], X[te_idx]\n",
    "        y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "        scaler = StandardScaler().fit(X_tr)\n",
    "        X_tr_s = scaler.transform(X_tr)[..., np.newaxis]\n",
    "        X_te_s = scaler.transform(X_te)[..., np.newaxis]\n",
    "        model = build_cnn(X_tr.shape[1])\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        model.fit(\n",
    "            X_tr_s, y_tr,\n",
    "            validation_data=(X_te_s, y_te),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[callbacks.EarlyStopping(patience=2, restore_best_weights=True)],\n",
    "            verbose=0\n",
    "        )\n",
    "        preds = model.predict(X_te_s).flatten()\n",
    "        fold_metrics.append(metrics(y_te, preds))\n",
    "        tf.keras.backend.clear_session()\n",
    "    return {k: np.mean([fm[k] for fm in fold_metrics]) for k in fold_metrics[0]}\n",
    "\n",
    "# RNN builder and CV\n",
    "def build_rnn(n_features):\n",
    "    return Sequential([\n",
    "        layers.Input(shape=(1, n_features)),\n",
    "        layers.SimpleRNN(32, activation='tanh'),\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "def rnn_cv(X, y, n_splits=3, epochs=10, batch_size=256):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    fold_metrics = []\n",
    "    for tr_idx, te_idx in tscv.split(X):\n",
    "        X_tr, X_te = X[tr_idx], X[te_idx]\n",
    "        y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "        scaler = StandardScaler().fit(X_tr)\n",
    "        X_tr_s = scaler.transform(X_tr).reshape(-1, 1, X_tr.shape[1])\n",
    "        X_te_s = scaler.transform(X_te).reshape(-1, 1, X_te.shape[1])\n",
    "        model = build_rnn(X_tr.shape[1])\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        model.fit(\n",
    "            X_tr_s, y_tr,\n",
    "            validation_data=(X_te_s, y_te),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[callbacks.EarlyStopping(patience=2, restore_best_weights=True)],\n",
    "            verbose=0\n",
    "        )\n",
    "        preds = model.predict(X_te_s).flatten()\n",
    "        fold_metrics.append(metrics(y_te, preds))\n",
    "        tf.keras.backend.clear_session()\n",
    "    return {k: np.mean([fm[k] for fm in fold_metrics]) for k in fold_metrics[0]}\n",
    "\n",
    "# 5) PINN: Physics-Informed NN enforcing output smoothness\n",
    "class PINN(Model):\n",
    "    def __init__(self, n_features, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.d1 = layers.Dense(64, activation='tanh')\n",
    "        self.d2 = layers.Dense(64, activation='tanh')\n",
    "        self.out = layers.Dense(1)\n",
    "        self.alpha = alpha\n",
    "    def call(self, x):\n",
    "        z = self.d1(x)\n",
    "        z = self.d2(z)\n",
    "        return self.out(z)\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)\n",
    "            data_loss = tf.reduce_mean(tf.square(y - y_pred))\n",
    "            with tf.GradientTape() as g2:\n",
    "                g2.watch(x)\n",
    "                y_hat = self(x)\n",
    "            dy_dx = g2.gradient(y_hat, x)\n",
    "            phys_loss = tf.reduce_mean(tf.square(dy_dx))\n",
    "            loss = data_loss + self.alpha * phys_loss\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        return {'loss': loss, 'data_loss': data_loss, 'phys_loss': phys_loss}\n",
    "\n",
    "def pinn_cv(X, y, n_splits=3, epochs=10, batch_size=256, alpha=1.0):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    fold_metrics = []\n",
    "    for tr_idx, te_idx in tscv.split(X):\n",
    "        X_tr, X_te = X[tr_idx], X[te_idx]\n",
    "        y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "        scaler = StandardScaler().fit(X_tr)\n",
    "        X_tr_s = scaler.transform(X_tr)\n",
    "        X_te_s = scaler.transform(X_te)\n",
    "        model = PINN(X_tr.shape[1], alpha)\n",
    "        model.compile(optimizer='adam')\n",
    "        model.fit(X_tr_s, y_tr, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "        preds = model.predict(X_te_s).flatten()\n",
    "        fold_metrics.append(metrics(y_te, preds))\n",
    "        tf.keras.backend.clear_session()\n",
    "    return {k: np.mean([fm[k] for fm in fold_metrics]) for k in fold_metrics[0]}\n",
    "\n",
    "# 6) Stacked Ensemble\n",
    "\n",
    "def stacked_cv(X, y, n_splits=3):\n",
    "    estimators = [\n",
    "        ('bayes', BayesianRidge()),\n",
    "        ('xgb', xgb.XGBRegressor(n_estimators=30, max_depth=3, learning_rate=0.1, objective='reg:squarederror', verbosity=0)),\n",
    "        ('ada', AdaBoostRegressor(estimator=DecisionTreeRegressor(max_depth=3), n_estimators=20, learning_rate=0.1, random_state=42))\n",
    "    ]\n",
    "    stack = StackingRegressor(estimators=estimators, final_estimator=BayesianRidge())\n",
    "    return cv_sklearn_model(stack, X, y)\n",
    "\n",
    "# 7) RUN ALL MODELS\n",
    "\n",
    "def run_all(data_dict):\n",
    "    records = []\n",
    "    for coin, df in data_dict.items():\n",
    "        X, y = prepare_xy(df, coin)\n",
    "        records.append({'coin': coin, 'model': 'BayesianRidge', **bayesian_ridge_cv(X, y)})\n",
    "        records.append({'coin': coin, 'model': 'XGBoost', **xgboost_cv(X, y)})\n",
    "        records.append({'coin': coin, 'model': 'AdaBoost', **adaboost_cv(X, y)})\n",
    "        records.append({'coin': coin, 'model': 'MLP', **mlp_cv(X, y)})\n",
    "        records.append({'coin': coin, 'model': 'CNN', **cnn_cv(X, y)})\n",
    "        records.append({'coin': coin, 'model': 'RNN', **rnn_cv(X, y)})\n",
    "        records.append({'coin': coin, 'model': 'PINN', **pinn_cv(X, y)})\n",
    "        records.append({'coin': coin, 'model': 'Stacked', **stacked_cv(X, y)})\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == '__main__':\n",
    "    results = run_all(data)\n",
    "    print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9d4e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 487us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 495us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 644us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 461us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 465us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 398us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 605us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 333us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 908us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 345us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 567us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 467us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 483us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 905us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 671us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 578us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 373us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 910us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 292us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 965us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 582us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# crypto_time_series_forecast.py\n",
    "# ---------------------------------------\n",
    "# Time-Series Forecasting of Crypto Returns\n",
    "# Models: PINN, CNN, RNN, MLP, BayesianRidge, XGBoost, AdaBoost, Stacked Ensemble\n",
    "# Uses TimeSeriesSplit for robust time-series cross-validation on one-minute data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.ensemble import AdaBoostRegressor, StackingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Sequential, callbacks\n",
    "\n",
    "# 1) LOAD DATA\n",
    "paths = {\n",
    "    'ada':   '/Users/jadenfix/downloads/processed_data_ada.csv',\n",
    "    'solana':'/Users/jadenfix/downloads/processed_data_solana.csv',\n",
    "    'eth':   '/Users/jadenfix/downloads/processed_data_eth.csv',\n",
    "    'btc':   '/Users/jadenfix/downloads/processed_data_btc.csv'\n",
    "}\n",
    "data = {coin: pd.read_csv(path, parse_dates=['timestamp']) for coin, path in paths.items()}\n",
    "\n",
    "# 2) UTIL FUNCTIONS\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute regression metrics including MAPE with safe handling of zero targets.\n",
    "    \"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    # Avoid division by zero in MAPE\n",
    "    mask = np.abs(y_true) > 1e-8\n",
    "    if np.any(mask):\n",
    "        mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "    else:\n",
    "        mape = np.nan\n",
    "    return {'mae': mae, 'rmse': rmse, 'r2': r2, 'mape': mape}\n",
    "\n",
    "# Generic TimeSeries CV for sklearn-style models\n",
    "\n",
    "def cv_sklearn_model(model, X, y, n_splits=3):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    fold_metrics = []\n",
    "    for train_idx, test_idx in tscv.split(X):\n",
    "        X_tr, X_te = X[train_idx], X[test_idx]\n",
    "        y_tr, y_te = y[train_idx], y[test_idx]\n",
    "        model.fit(X_tr, y_tr)\n",
    "        preds = model.predict(X_te)\n",
    "        fold_metrics.append(metrics(y_te, preds))\n",
    "    return {k: np.mean([fm[k] for fm in fold_metrics]) for k in fold_metrics[0]}\n",
    "\n",
    "# Prepare features/target\n",
    "def prepare_xy(df, coin):\n",
    "    df = df.sort_values('timestamp').dropna()\n",
    "    X = df.drop(columns=[f'{coin}_r_next', 'timestamp']).values\n",
    "    y = df[f'{coin}_r_next'].values\n",
    "    return X, y\n",
    "\n",
    "# 3) SKLEARN MODEL FUNCTIONS\n",
    "\n",
    "def bayesian_ridge_cv(X, y):\n",
    "    return cv_sklearn_model(BayesianRidge(), X, y)\n",
    "\n",
    "\n",
    "def xgboost_cv(X, y):\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=50,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        objective='reg:squarederror',\n",
    "        verbosity=0\n",
    "    )\n",
    "    return cv_sklearn_model(model, X, y)\n",
    "\n",
    "\n",
    "def adaboost_cv(X, y):\n",
    "    base = DecisionTreeRegressor(max_depth=3)\n",
    "    model = AdaBoostRegressor(\n",
    "        estimator=base,\n",
    "        n_estimators=30,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "    return cv_sklearn_model(model, X, y)\n",
    "\n",
    "# 4) NEURAL NET MODELS\n",
    "\n",
    "# MLP\n",
    "\n",
    "def build_mlp(n_features):\n",
    "    return Sequential([\n",
    "        layers.Input(shape=(n_features,)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "def mlp_cv(X, y, n_splits=3, epochs=10, batch_size=256):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    fold_metrics = []\n",
    "    for tr_idx, te_idx in tscv.split(X):\n",
    "        X_tr, X_te = X[tr_idx], X[te_idx]\n",
    "        y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "        scaler = StandardScaler().fit(X_tr)\n",
    "        X_tr_s = scaler.transform(X_tr)\n",
    "        X_te_s = scaler.transform(X_te)\n",
    "        model = build_mlp(X_tr_s.shape[1])\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        model.fit(\n",
    "            X_tr_s, y_tr,\n",
    "            validation_data=(X_te_s, y_te),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[callbacks.EarlyStopping(patience=2, restore_best_weights=True)],\n",
    "            verbose=0\n",
    "        )\n",
    "        preds = model.predict(X_te_s).flatten()\n",
    "        fold_metrics.append(metrics(y_te, preds))\n",
    "        tf.keras.backend.clear_session()\n",
    "    return {k: np.mean([fm[k] for fm in fold_metrics]) for k in fold_metrics[0]}\n",
    "\n",
    "# CNN\n",
    "\n",
    "def build_cnn(n_features):\n",
    "    return Sequential([\n",
    "        layers.Input(shape=(n_features, 1)),\n",
    "        layers.Conv1D(32, kernel_size=3, activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "def cnn_cv(X, y, n_splits=3, epochs=10, batch_size=256):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    fold_metrics = []\n",
    "    for tr_idx, te_idx in tscv.split(X):\n",
    "        X_tr, X_te = X[tr_idx], X[te_idx]\n",
    "        y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "        scaler = StandardScaler().fit(X_tr)\n",
    "        X_tr_s = scaler.transform(X_tr)[..., np.newaxis]\n",
    "        X_te_s = scaler.transform(X_te)[..., np.newaxis]\n",
    "        model = build_cnn(X_tr.shape[1])\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        model.fit(\n",
    "            X_tr_s, y_tr,\n",
    "            validation_data=(X_te_s, y_te),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[callbacks.EarlyStopping(patience=2, restore_best_weights=True)],\n",
    "            verbose=0\n",
    "        )\n",
    "        preds = model.predict(X_te_s).flatten()\n",
    "        fold_metrics.append(metrics(y_te, preds))\n",
    "        tf.keras.backend.clear_session()\n",
    "    return {k: np.mean([fm[k] for fm in fold_metrics]) for k in fold_metrics[0]}\n",
    "\n",
    "# RNN\n",
    "\n",
    "def build_rnn(n_features):\n",
    "    return Sequential([\n",
    "        layers.Input(shape=(1, n_features)),\n",
    "        layers.SimpleRNN(32, activation='tanh'),\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "def rnn_cv(X, y, n_splits=3, epochs=10, batch_size=256):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    fold_metrics = []\n",
    "    for tr_idx, te_idx in tscv.split(X):\n",
    "        X_tr, X_te = X[tr_idx], X[te_idx]\n",
    "        y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "        scaler = StandardScaler().fit(X_tr)\n",
    "        X_tr_s = scaler.transform(X_tr).reshape(-1, 1, X_tr.shape[1])\n",
    "        X_te_s = scaler.transform(X_te).reshape(-1, 1, X_te.shape[1])\n",
    "        model = build_rnn(X_tr.shape[1])\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        model.fit(\n",
    "            X_tr_s, y_tr,\n",
    "            validation_data=(X_te_s, y_te),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[callbacks.EarlyStopping(patience=2, restore_best_weights=True)],\n",
    "            verbose=0\n",
    "        )\n",
    "        preds = model.predict(X_te_s).flatten()\n",
    "        fold_metrics.append(metrics(y_te, preds))\n",
    "        tf.keras.backend.clear_session()\n",
    "    return {k: np.mean([fm[k] for fm in fold_metrics]) for k in fold_metrics[0]}\n",
    "\n",
    "# 5) PINN: Physics-Informed NN enforcing smoothness\n",
    "class PINN(Model):\n",
    "    def __init__(self, n_features, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.d1 = layers.Dense(64, activation='tanh')\n",
    "        self.d2 = layers.Dense(64, activation='tanh')\n",
    "        self.out = layers.Dense(1)\n",
    "        self.alpha = alpha\n",
    "    def call(self, x):\n",
    "        z = self.d1(x)\n",
    "        z = self.d2(z)\n",
    "        return self.out(z)\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)\n",
    "            data_loss = tf.reduce_mean(tf.square(y - y_pred))\n",
    "            with tf.GradientTape() as g2:\n",
    "                g2.watch(x)\n",
    "                y_hat = self(x)\n",
    "            dy_dx = g2.gradient(y_hat, x)\n",
    "            phys_loss = tf.reduce_mean(tf.square(dy_dx))\n",
    "            loss = data_loss + self.alpha * phys_loss\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        return {'loss': loss, 'data_loss': data_loss, 'phys_loss': phys_loss}\n",
    "\n",
    "def pinn_cv(X, y, n_splits=3, epochs=10, batch_size=256, alpha=1.0):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    fold_metrics = []\n",
    "    for tr_idx, te_idx in tscv.split(X):\n",
    "        X_tr, X_te = X[tr_idx], X[te_idx]\n",
    "        y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "        scaler = StandardScaler().fit(X_tr)\n",
    "        X_tr_s = scaler.transform(X_tr)\n",
    "        X_te_s = scaler.transform(X_te)\n",
    "        model = PINN(X_tr.shape[1], alpha)\n",
    "        model.compile(optimizer='adam')\n",
    "        model.fit(X_tr_s, y_tr, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "        preds = model.predict(X_te_s).flatten()\n",
    "        fold_metrics.append(metrics(y_te, preds))\n",
    "        tf.keras.backend.clear_session()\n",
    "    return {k: np.mean([fm[k] for fm in fold_metrics]) for k in fold_metrics[0]}\n",
    "\n",
    "# 6) Stacked Ensemble\n",
    "def stacked_cv(X, y, n_splits=3):\n",
    "    estimators = [\n",
    "        ('bayes', BayesianRidge()),\n",
    "        ('xgb', xgb.XGBRegressor(n_estimators=30, max_depth=3, learning_rate=0.1, objective='reg:squarederror', verbosity=0)),\n",
    "        ('ada', AdaBoostRegressor(estimator=DecisionTreeRegressor(max_depth=3), n_estimators=20, learning_rate=0.1, random_state=42))\n",
    "    ]\n",
    "    stack = StackingRegressor(estimators=estimators, final_estimator=BayesianRidge())\n",
    "    return cv_sklearn_model(stack, X, y)\n",
    "\n",
    "# 7) RUN ALL MODELS\n",
    "def run_all(data_dict):\n",
    "    records = []\n",
    "    for coin, df in data_dict.items():\n",
    "        X, y = prepare_xy(df, coin)\n",
    "        records.append({'coin': coin, 'model': 'BayesianRidge', **bayesian_ridge_cv(X, y)})\n",
    "        records.append({'coin': coin, 'model': 'XGBoost', **xgboost_cv(X, y)})\n",
    "        records.append({'coin': coin, 'model': 'AdaBoost', **adaboost_cv(X, y)})\n",
    "        records.append({'coin': coin, 'model': 'MLP', **mlp_cv(X, y)})\n",
    "        records.append({'coin': coin, 'model': 'CNN', **cnn_cv(X, y)})\n",
    "        records.append({'coin': coin, 'model': 'RNN', **rnn_cv(X, y)})\n",
    "        records.append({'coin': coin, 'model': 'PINN', **pinn_cv(X, y)})\n",
    "        records.append({'coin': coin, 'model': 'Stacked', **stacked_cv(X, y)})\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == '__main__':\n",
    "    results = run_all(data)\n",
    "    print(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
