{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35118933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (2.1.3)\n",
      "Requirement already satisfied: pandas in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (1.6.1)\n",
      "Requirement already satisfied: xgboost in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (3.0.1)\n",
      "Requirement already satisfied: tensorflow in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (2.19.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (3.9.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: namex in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
      "Requirement already satisfied: optree in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas scikit-learn xgboost tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3d870c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cec9288",
   "metadata": {},
   "source": [
    "For ada solana and eth: Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1c4cdbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Users/jadenfix/downloads/processed_data_ada_long.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m data_ada_long = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mUsers/jadenfix/downloads/processed_data_ada_long.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAda long:\u001b[39m\u001b[33m\"\u001b[39m,data_ada_long.head())\n\u001b[32m      4\u001b[39m data_solana_long = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33mUsers/jadenfix/downloads/processed_data_solana_long.csv\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'Users/jadenfix/downloads/processed_data_ada_long.csv'"
     ]
    }
   ],
   "source": [
    "data_ada_long = pd.read_csv('Users/jadenfix/downloads/processed_data_ada_long.csv')\n",
    "print(\"Ada long:\",data_ada_long.head())\n",
    "\n",
    "data_solana_long = pd.read_csv('Users/jadenfix/downloads/processed_data_solana_long.csv')\n",
    "print(\"Solana long:\",data_solana_long.head())\n",
    "\n",
    "data_eth_long = pd.read_csv('Users/jadenfix/downloads/processed_data_eth_long.csv')\n",
    "print(\"ETH long:\",data_eth_long.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7776b677",
   "metadata": {},
   "source": [
    "For short: ada btc eth and solana\n",
    "/Users/jadenfix/Downloads/processed_data_solana.csv /Users/jadenfix/Downloads/processed_data_ada.csv /Users/jadenfix/Downloads/processed_data_eth.csv /Users/jadenfix/Downloads/processed_data_btc.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dac83b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ada short:    btc_open  btc_high   btc_low  btc_close  btc_volume  eth_open  eth_high  \\\n",
      "0  29050.29  29050.29  29050.29   29050.29         0.0   1829.91   1830.50   \n",
      "1  29050.29  29050.29  29050.29   29050.29         0.0   1830.49   1830.68   \n",
      "2  29050.29  29050.29  29050.29   29050.29         0.0   1830.67   1830.76   \n",
      "3  29050.29  29050.29  29050.29   29050.29         0.0   1830.76   1830.93   \n",
      "4  29050.29  29050.29  29050.29   29050.29         0.0   1830.92   1831.17   \n",
      "\n",
      "   eth_low  eth_close  eth_volume  ...  solana_close_lag3  \\\n",
      "0  1829.53    1830.49    362.8306  ...              22.79   \n",
      "1  1830.49    1830.68     29.8070  ...              22.77   \n",
      "2  1830.67    1830.75     28.3587  ...              22.77   \n",
      "3  1830.75    1830.92     14.4738  ...              22.77   \n",
      "4  1830.92    1831.17     82.1231  ...              22.78   \n",
      "\n",
      "                    timestamp   ada_log     ada_r  ada_r_next  ada_r_winsor  \\\n",
      "0  2023-08-05T00:03:00.000000 -1.225878  0.000000    0.000341      0.000000   \n",
      "1  2023-08-05T00:04:00.000000 -1.225537  0.000341   -0.000681      0.000341   \n",
      "2  2023-08-05T00:05:00.000000 -1.226218 -0.000681    0.000341     -0.000681   \n",
      "3  2023-08-05T00:06:00.000000 -1.225878  0.000341    0.000681      0.000341   \n",
      "4  2023-08-05T00:07:00.000000 -1.225196  0.000681   -0.000681      0.000681   \n",
      "\n",
      "   ada_r_ma1h  ada_r_vol1h  ada_r_ma1d  ada_r_vol1d  \n",
      "0    0.000114     0.000197    0.000114     0.000197  \n",
      "1    0.000170     0.000197    0.000170     0.000197  \n",
      "2    0.000000     0.000417    0.000000     0.000417  \n",
      "3    0.000057     0.000398    0.000057     0.000398  \n",
      "4    0.000146     0.000433    0.000146     0.000433  \n",
      "\n",
      "[5 rows x 42 columns]\n",
      "Solana short:    btc_open  btc_high   btc_low  btc_close  btc_volume  eth_open  eth_high  \\\n",
      "0  29050.29  29050.29  29050.29   29050.29         0.0   1829.91   1830.50   \n",
      "1  29050.29  29050.29  29050.29   29050.29         0.0   1830.49   1830.68   \n",
      "2  29050.29  29050.29  29050.29   29050.29         0.0   1830.67   1830.76   \n",
      "3  29050.29  29050.29  29050.29   29050.29         0.0   1830.76   1830.93   \n",
      "4  29050.29  29050.29  29050.29   29050.29         0.0   1830.92   1831.17   \n",
      "\n",
      "   eth_low  eth_close  eth_volume  ...  solana_close_lag3  \\\n",
      "0  1829.53    1830.49    362.8306  ...              22.79   \n",
      "1  1830.49    1830.68     29.8070  ...              22.77   \n",
      "2  1830.67    1830.75     28.3587  ...              22.77   \n",
      "3  1830.75    1830.92     14.4738  ...              22.77   \n",
      "4  1830.92    1831.17     82.1231  ...              22.78   \n",
      "\n",
      "                    timestamp  solana_log  solana_r  solana_r_next  \\\n",
      "0  2023-08-05T00:03:00.000000    3.125444  0.000000       0.000439   \n",
      "1  2023-08-05T00:04:00.000000    3.125883  0.000439       0.000000   \n",
      "2  2023-08-05T00:05:00.000000    3.125883  0.000000       0.000439   \n",
      "3  2023-08-05T00:06:00.000000    3.126322  0.000439       0.000877   \n",
      "4  2023-08-05T00:07:00.000000    3.127199  0.000877       0.000438   \n",
      "\n",
      "   solana_r_winsor  solana_r_ma1h  solana_r_vol1h  solana_r_ma1d  \\\n",
      "0         0.000000      -0.000293        0.000507      -0.000293   \n",
      "1         0.000439      -0.000110        0.000552      -0.000110   \n",
      "2         0.000000      -0.000088        0.000481      -0.000088   \n",
      "3         0.000439       0.000000        0.000481       0.000000   \n",
      "4         0.000877       0.000125        0.000550       0.000125   \n",
      "\n",
      "   solana_r_vol1d  \n",
      "0        0.000507  \n",
      "1        0.000552  \n",
      "2        0.000481  \n",
      "3        0.000481  \n",
      "4        0.000550  \n",
      "\n",
      "[5 rows x 42 columns]\n",
      "ETH short:    btc_open  btc_high   btc_low  btc_close  btc_volume  eth_open  eth_high  \\\n",
      "0  29050.29  29050.29  29050.29   29050.29         0.0   1829.91   1830.50   \n",
      "1  29050.29  29050.29  29050.29   29050.29         0.0   1830.49   1830.68   \n",
      "2  29050.29  29050.29  29050.29   29050.29         0.0   1830.67   1830.76   \n",
      "3  29050.29  29050.29  29050.29   29050.29         0.0   1830.76   1830.93   \n",
      "4  29050.29  29050.29  29050.29   29050.29         0.0   1830.92   1831.17   \n",
      "\n",
      "   eth_low  eth_close  eth_volume  ...  solana_close_lag3  \\\n",
      "0  1829.53    1830.49    362.8306  ...              22.79   \n",
      "1  1830.49    1830.68     29.8070  ...              22.77   \n",
      "2  1830.67    1830.75     28.3587  ...              22.77   \n",
      "3  1830.75    1830.92     14.4738  ...              22.77   \n",
      "4  1830.92    1831.17     82.1231  ...              22.78   \n",
      "\n",
      "                    timestamp   eth_log     eth_r  eth_r_next  eth_r_winsor  \\\n",
      "0  2023-08-05T00:03:00.000000  7.512339  0.000311    0.000104      0.000311   \n",
      "1  2023-08-05T00:04:00.000000  7.512443  0.000104    0.000038      0.000104   \n",
      "2  2023-08-05T00:05:00.000000  7.512481  0.000038    0.000093      0.000038   \n",
      "3  2023-08-05T00:06:00.000000  7.512574  0.000093    0.000137      0.000093   \n",
      "4  2023-08-05T00:07:00.000000  7.512710  0.000137   -0.000027      0.000137   \n",
      "\n",
      "   eth_r_ma1h  eth_r_vol1h  eth_r_ma1d  eth_r_vol1d  \n",
      "0    0.000104     0.000180    0.000104     0.000180  \n",
      "1    0.000104     0.000147    0.000104     0.000147  \n",
      "2    0.000091     0.000130    0.000091     0.000130  \n",
      "3    0.000091     0.000117    0.000091     0.000117  \n",
      "4    0.000098     0.000108    0.000098     0.000108  \n",
      "\n",
      "[5 rows x 42 columns]\n",
      "BTC short:    btc_open  btc_high   btc_low  btc_close  btc_volume  eth_open  eth_high  \\\n",
      "0  29050.29  29050.29  29050.29   29050.29         0.0   1829.91   1830.50   \n",
      "1  29050.29  29050.29  29050.29   29050.29         0.0   1830.49   1830.68   \n",
      "2  29050.29  29050.29  29050.29   29050.29         0.0   1830.67   1830.76   \n",
      "3  29050.29  29050.29  29050.29   29050.29         0.0   1830.76   1830.93   \n",
      "4  29050.29  29050.29  29050.29   29050.29         0.0   1830.92   1831.17   \n",
      "\n",
      "   eth_low  eth_close  eth_volume  ...  solana_close_lag3  \\\n",
      "0  1829.53    1830.49    362.8306  ...              22.79   \n",
      "1  1830.49    1830.68     29.8070  ...              22.77   \n",
      "2  1830.67    1830.75     28.3587  ...              22.77   \n",
      "3  1830.75    1830.92     14.4738  ...              22.77   \n",
      "4  1830.92    1831.17     82.1231  ...              22.78   \n",
      "\n",
      "                    timestamp    btc_log  btc_r  btc_r_next  btc_r_winsor  \\\n",
      "0  2023-08-05T00:03:00.000000  10.276784    0.0         0.0           0.0   \n",
      "1  2023-08-05T00:04:00.000000  10.276784    0.0         0.0           0.0   \n",
      "2  2023-08-05T00:05:00.000000  10.276784    0.0         0.0           0.0   \n",
      "3  2023-08-05T00:06:00.000000  10.276784    0.0         0.0           0.0   \n",
      "4  2023-08-05T00:07:00.000000  10.276784    0.0         0.0           0.0   \n",
      "\n",
      "   btc_r_ma1h  btc_r_vol1h  btc_r_ma1d  btc_r_vol1d  \n",
      "0         0.0          0.0         0.0          0.0  \n",
      "1         0.0          0.0         0.0          0.0  \n",
      "2         0.0          0.0         0.0          0.0  \n",
      "3         0.0          0.0         0.0          0.0  \n",
      "4         0.0          0.0         0.0          0.0  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "data_ada_short = pd.read_csv('Users/jadenfix/downloads/processed_data_ada.csv')\n",
    "print(\"Ada short:\",data_ada_short.head())\n",
    "data_solana_short = pd.read_csv('Users/jadenfix/downloads/processed_data_solana.csv')\n",
    "print(\"Solana short:\",data_solana_short.head())\n",
    "data_eth_short = pd.read_csv('Users/jadenfix/downloads/processed_data_eth.csv')\n",
    "print(\"ETH short:\",data_eth_short.head())\n",
    "data_btc_short = pd.read_csv('Users/jadenfix/downloads/processed_data_btc.csv')\n",
    "print(\"BTC short:\",data_btc_short.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1eb84d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 964us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 726us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 453us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 343us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 292us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 403us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 298us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 448us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 458us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 494us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 505us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 562us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 482us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 284us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 421us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 316us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 571us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 300us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 383us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 410us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 429us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 350us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 306us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 323us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 355us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 357us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 346us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 291us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 317us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 315us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 293us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265us/step\n",
      "      coin          model       mae      rmse            r2  mape\n",
      "0      ada  BayesianRidge  0.000314  0.000526      0.732095   inf\n",
      "1      ada        XGBoost  0.001183  0.001599     -2.739989   inf\n",
      "2      ada       AdaBoost  0.000723  0.001211     -0.807472   inf\n",
      "3      ada            MLP  0.032039  0.048455  -7404.689256   inf\n",
      "4      ada            CNN  0.009489  0.017212   -935.336309   inf\n",
      "5      ada            RNN  0.070889  0.087756 -14401.782840   inf\n",
      "6      ada           PINN  0.041914  0.053716  -6379.258573   inf\n",
      "7      ada        Stacked  0.000258  0.000451      0.823594   inf\n",
      "8   solana  BayesianRidge  0.000823  0.001402      0.085203   inf\n",
      "9   solana        XGBoost  0.001076  0.001549     -0.073499   inf\n",
      "10  solana       AdaBoost  0.001082  0.001778     -0.731032   inf\n",
      "11  solana            MLP  0.038952  0.052468  -2031.786506   inf\n",
      "12  solana            CNN  0.026064  0.040224   -922.144412   inf\n",
      "13  solana            RNN  0.048388  0.060126  -5211.131619   inf\n",
      "14  solana           PINN  0.025214  0.034335   -926.385693   inf\n",
      "15  solana        Stacked  0.000597  0.001088      0.381056   inf\n",
      "16     eth  BayesianRidge  0.000065  0.000108      0.967003   inf\n",
      "17     eth        XGBoost  0.000530  0.000870     -1.267007   inf\n",
      "18     eth       AdaBoost  0.000401  0.000639     -0.022713   inf\n",
      "19     eth            MLP  0.067586  0.091958 -29132.873610   inf\n",
      "20     eth            CNN  0.032021  0.045348  -8033.626875   inf\n",
      "21     eth            RNN  0.069068  0.090079 -27590.104049   inf\n",
      "22     eth           PINN  0.018299  0.024816  -2263.332846   inf\n",
      "23     eth        Stacked  0.000067  0.000109      0.969568   inf\n",
      "24     btc  BayesianRidge  0.000087  0.000142      0.925084   inf\n",
      "25     btc        XGBoost  0.000401  0.000631     -0.249700   inf\n",
      "26     btc       AdaBoost  0.000354  0.000575     -0.005809   inf\n",
      "27     btc            MLP  0.061784  0.084336 -45229.280229   inf\n",
      "28     btc            CNN  0.010191  0.014663  -1544.681032   inf\n",
      "29     btc            RNN  0.029534  0.039574  -5145.440101   inf\n",
      "30     btc           PINN  0.025018  0.037879  -8470.980382   inf\n",
      "31     btc        Stacked  0.000084  0.000137      0.931765   inf\n"
     ]
    }
   ],
   "source": [
    "# crypto_time_series_forecast.py\n",
    "# ---------------------------------------\n",
    "# Time-Series Forecasting of Crypto Returns\n",
    "# Models: PINN, CNN, RNN, MLP, BayesianRidge, XGBoost, AdaBoost, Stacked Ensemble\n",
    "# Uses TimeSeriesSplit for robust time-series cross-validation on one-minute data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.ensemble import AdaBoostRegressor, StackingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Sequential, callbacks\n",
    "\n",
    "# 1) LOAD DATA\n",
    "paths = {\n",
    "    'ada':   '/Users/jadenfix/downloads/processed_data_ada.csv',\n",
    "    'solana':'/Users/jadenfix/downloads/processed_data_solana.csv',\n",
    "    'eth':   '/Users/jadenfix/downloads/processed_data_eth.csv',\n",
    "    'btc':   '/Users/jadenfix/downloads/processed_data_btc.csv'\n",
    "}\n",
    "# parse timestamp\n",
    "data = {coin: pd.read_csv(path, parse_dates=['timestamp']) for coin, path in paths.items()}\n",
    "\n",
    "# 2) UTIL FUNCTIONS\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    # Adds MAPE for percent error insight\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    return {'mae': mae, 'rmse': rmse, 'r2': r2, 'mape': mape}\n",
    "\n",
    "# Generic TS CV for sklearn-style models\n",
    "def cv_sklearn_model(model, X, y, n_splits=3):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    fold_metrics = []\n",
    "    for train_idx, test_idx in tscv.split(X):\n",
    "        X_tr, X_te = X[train_idx], X[test_idx]\n",
    "        y_tr, y_te = y[train_idx], y[test_idx]\n",
    "        model.fit(X_tr, y_tr)\n",
    "        preds = model.predict(X_te)\n",
    "        fold_metrics.append(metrics(y_te, preds))\n",
    "    return {k: np.mean([fm[k] for fm in fold_metrics]) for k in fold_metrics[0]}\n",
    "\n",
    "# Prepare features/target\n",
    "def prepare_xy(df, coin):\n",
    "    df = df.sort_values('timestamp').dropna()\n",
    "    X = df.drop(columns=[f'{coin}_r_next', 'timestamp']).values\n",
    "    y = df[f'{coin}_r_next'].values\n",
    "    return X, y\n",
    "\n",
    "# 3) SKLEARN MODEL FUNCTIONS\n",
    "\n",
    "def bayesian_ridge_cv(X, y):\n",
    "    return cv_sklearn_model(BayesianRidge(), X, y)\n",
    "\n",
    "\n",
    "def xgboost_cv(X, y):\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=50,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        objective='reg:squarederror',\n",
    "        verbosity=0\n",
    "    )\n",
    "    return cv_sklearn_model(model, X, y)\n",
    "\n",
    "\n",
    "def adaboost_cv(X, y):\n",
    "    base = DecisionTreeRegressor(max_depth=3)\n",
    "    model = AdaBoostRegressor(\n",
    "        estimator=base,\n",
    "        n_estimators=30,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "    return cv_sklearn_model(model, X, y)\n",
    "\n",
    "# 4) NEURAL-NET MODELS\n",
    "\n",
    "# MLP builder and CV\n",
    "def build_mlp(n_features):\n",
    "    return Sequential([\n",
    "        layers.Input(shape=(n_features,)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "def mlp_cv(X, y, n_splits=3, epochs=10, batch_size=256):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    fold_metrics = []\n",
    "    for tr_idx, te_idx in tscv.split(X):\n",
    "        X_tr, X_te = X[tr_idx], X[te_idx]\n",
    "        y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "        scaler = StandardScaler().fit(X_tr)\n",
    "        X_tr_s = scaler.transform(X_tr)\n",
    "        X_te_s = scaler.transform(X_te)\n",
    "        model = build_mlp(X_tr_s.shape[1])\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        model.fit(\n",
    "            X_tr_s, y_tr,\n",
    "            validation_data=(X_te_s, y_te),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[callbacks.EarlyStopping(patience=2, restore_best_weights=True)],\n",
    "            verbose=0\n",
    "        )\n",
    "        preds = model.predict(X_te_s).flatten()\n",
    "        fold_metrics.append(metrics(y_te, preds))\n",
    "        tf.keras.backend.clear_session()\n",
    "    return {k: np.mean([fm[k] for fm in fold_metrics]) for k in fold_metrics[0]}\n",
    "\n",
    "# CNN builder and CV\n",
    "def build_cnn(n_features):\n",
    "    return Sequential([\n",
    "        layers.Input(shape=(n_features, 1)),\n",
    "        layers.Conv1D(32, kernel_size=3, activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "def cnn_cv(X, y, n_splits=3, epochs=10, batch_size=256):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    fold_metrics = []\n",
    "    for tr_idx, te_idx in tscv.split(X):\n",
    "        X_tr, X_te = X[tr_idx], X[te_idx]\n",
    "        y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "        scaler = StandardScaler().fit(X_tr)\n",
    "        X_tr_s = scaler.transform(X_tr)[..., np.newaxis]\n",
    "        X_te_s = scaler.transform(X_te)[..., np.newaxis]\n",
    "        model = build_cnn(X_tr.shape[1])\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        model.fit(\n",
    "            X_tr_s, y_tr,\n",
    "            validation_data=(X_te_s, y_te),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[callbacks.EarlyStopping(patience=2, restore_best_weights=True)],\n",
    "            verbose=0\n",
    "        )\n",
    "        preds = model.predict(X_te_s).flatten()\n",
    "        fold_metrics.append(metrics(y_te, preds))\n",
    "        tf.keras.backend.clear_session()\n",
    "    return {k: np.mean([fm[k] for fm in fold_metrics]) for k in fold_metrics[0]}\n",
    "\n",
    "# RNN builder and CV\n",
    "def build_rnn(n_features):\n",
    "    return Sequential([\n",
    "        layers.Input(shape=(1, n_features)),\n",
    "        layers.SimpleRNN(32, activation='tanh'),\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "def rnn_cv(X, y, n_splits=3, epochs=10, batch_size=256):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    fold_metrics = []\n",
    "    for tr_idx, te_idx in tscv.split(X):\n",
    "        X_tr, X_te = X[tr_idx], X[te_idx]\n",
    "        y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "        scaler = StandardScaler().fit(X_tr)\n",
    "        X_tr_s = scaler.transform(X_tr).reshape(-1, 1, X_tr.shape[1])\n",
    "        X_te_s = scaler.transform(X_te).reshape(-1, 1, X_te.shape[1])\n",
    "        model = build_rnn(X_tr.shape[1])\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        model.fit(\n",
    "            X_tr_s, y_tr,\n",
    "            validation_data=(X_te_s, y_te),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[callbacks.EarlyStopping(patience=2, restore_best_weights=True)],\n",
    "            verbose=0\n",
    "        )\n",
    "        preds = model.predict(X_te_s).flatten()\n",
    "        fold_metrics.append(metrics(y_te, preds))\n",
    "        tf.keras.backend.clear_session()\n",
    "    return {k: np.mean([fm[k] for fm in fold_metrics]) for k in fold_metrics[0]}\n",
    "\n",
    "# 5) PINN: Physics-Informed NN enforcing output smoothness\n",
    "class PINN(Model):\n",
    "    def __init__(self, n_features, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.d1 = layers.Dense(64, activation='tanh')\n",
    "        self.d2 = layers.Dense(64, activation='tanh')\n",
    "        self.out = layers.Dense(1)\n",
    "        self.alpha = alpha\n",
    "    def call(self, x):\n",
    "        z = self.d1(x)\n",
    "        z = self.d2(z)\n",
    "        return self.out(z)\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)\n",
    "            data_loss = tf.reduce_mean(tf.square(y - y_pred))\n",
    "            with tf.GradientTape() as g2:\n",
    "                g2.watch(x)\n",
    "                y_hat = self(x)\n",
    "            dy_dx = g2.gradient(y_hat, x)\n",
    "            phys_loss = tf.reduce_mean(tf.square(dy_dx))\n",
    "            loss = data_loss + self.alpha * phys_loss\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        return {'loss': loss, 'data_loss': data_loss, 'phys_loss': phys_loss}\n",
    "\n",
    "def pinn_cv(X, y, n_splits=3, epochs=10, batch_size=256, alpha=1.0):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    fold_metrics = []\n",
    "    for tr_idx, te_idx in tscv.split(X):\n",
    "        X_tr, X_te = X[tr_idx], X[te_idx]\n",
    "        y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "        scaler = StandardScaler().fit(X_tr)\n",
    "        X_tr_s = scaler.transform(X_tr)\n",
    "        X_te_s = scaler.transform(X_te)\n",
    "        model = PINN(X_tr.shape[1], alpha)\n",
    "        model.compile(optimizer='adam')\n",
    "        model.fit(X_tr_s, y_tr, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "        preds = model.predict(X_te_s).flatten()\n",
    "        fold_metrics.append(metrics(y_te, preds))\n",
    "        tf.keras.backend.clear_session()\n",
    "    return {k: np.mean([fm[k] for fm in fold_metrics]) for k in fold_metrics[0]}\n",
    "\n",
    "# 6) Stacked Ensemble\n",
    "\n",
    "def stacked_cv(X, y, n_splits=3):\n",
    "    estimators = [\n",
    "        ('bayes', BayesianRidge()),\n",
    "        ('xgb', xgb.XGBRegressor(n_estimators=30, max_depth=3, learning_rate=0.1, objective='reg:squarederror', verbosity=0)),\n",
    "        ('ada', AdaBoostRegressor(estimator=DecisionTreeRegressor(max_depth=3), n_estimators=20, learning_rate=0.1, random_state=42))\n",
    "    ]\n",
    "    stack = StackingRegressor(estimators=estimators, final_estimator=BayesianRidge())\n",
    "    return cv_sklearn_model(stack, X, y)\n",
    "\n",
    "# 7) RUN ALL MODELS\n",
    "\n",
    "def run_all(data_dict):\n",
    "    records = []\n",
    "    for coin, df in data_dict.items():\n",
    "        X, y = prepare_xy(df, coin)\n",
    "        records.append({'coin': coin, 'model': 'BayesianRidge', **bayesian_ridge_cv(X, y)})\n",
    "        records.append({'coin': coin, 'model': 'XGBoost', **xgboost_cv(X, y)})\n",
    "        records.append({'coin': coin, 'model': 'AdaBoost', **adaboost_cv(X, y)})\n",
    "        records.append({'coin': coin, 'model': 'MLP', **mlp_cv(X, y)})\n",
    "        records.append({'coin': coin, 'model': 'CNN', **cnn_cv(X, y)})\n",
    "        records.append({'coin': coin, 'model': 'RNN', **rnn_cv(X, y)})\n",
    "        records.append({'coin': coin, 'model': 'PINN', **pinn_cv(X, y)})\n",
    "        records.append({'coin': coin, 'model': 'Stacked', **stacked_cv(X, y)})\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == '__main__':\n",
    "    results = run_all(data)\n",
    "    print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b9d4e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 344us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 456us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 606us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 548us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 454us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 307us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 447us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 303us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 812us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 306us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 341us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 789us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 381us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 436us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 359us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 373us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 298us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 303us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 299us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 276us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 273us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 321us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 958us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 445us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 518us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 896us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 532us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 422us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m358s\u001b[0m 213ms/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 301us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 273us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 428us/step\n",
      "\u001b[1m1677/1677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 310us/step\n",
      "      coin          model       mae      rmse            r2           mape\n",
      "0      ada  BayesianRidge  0.000314  0.000526      0.732095      33.990279\n",
      "1      ada        XGBoost  0.001183  0.001599     -2.739989     206.251530\n",
      "2      ada       AdaBoost  0.000723  0.001211     -0.807472     105.942868\n",
      "3      ada            MLP  0.070386  0.098032 -31442.267688   13671.557335\n",
      "4      ada            CNN  0.024308  0.031540  -1739.938525    4543.628443\n",
      "5      ada            RNN  0.040818  0.051477  -3745.559233    7402.451983\n",
      "6      ada           PINN  0.024247  0.032009  -1184.789068    4578.218482\n",
      "7      ada        Stacked  0.000258  0.000451      0.823594      28.851613\n",
      "8   solana  BayesianRidge  0.000823  0.001402      0.085203      64.605613\n",
      "9   solana        XGBoost  0.001076  0.001549     -0.073499     108.259550\n",
      "10  solana       AdaBoost  0.001082  0.001778     -0.731032     106.748327\n",
      "11  solana            MLP  0.074855  0.111412 -18836.056873   11390.807115\n",
      "12  solana            CNN  0.006525  0.010846   -112.517406    1093.626792\n",
      "13  solana            RNN  0.134050  0.157579 -16465.174191   20508.278650\n",
      "14  solana           PINN  0.026724  0.036610  -1591.133041    4094.849853\n",
      "15  solana        Stacked  0.000597  0.001088      0.381056      47.150600\n",
      "16     eth  BayesianRidge  0.000065  0.000108      0.967003      16.488303\n",
      "17     eth        XGBoost  0.000530  0.000870     -1.267007     396.507654\n",
      "18     eth       AdaBoost  0.000401  0.000639     -0.022713     103.388628\n",
      "19     eth            MLP  0.057049  0.076419 -22220.664679   75682.278121\n",
      "20     eth            CNN  0.004139  0.008492   -376.086220    4674.199564\n",
      "21     eth            RNN  0.051875  0.072173 -19329.082415   62816.718338\n",
      "22     eth           PINN  0.041911  0.052829  -9150.449054   50718.934732\n",
      "23     eth        Stacked  0.000067  0.000109      0.969568      33.717870\n",
      "24     btc  BayesianRidge  0.000087  0.000142      0.925084      37.063542\n",
      "25     btc        XGBoost  0.000401  0.000631     -0.249700     313.160574\n",
      "26     btc       AdaBoost  0.000354  0.000575     -0.005809     102.858757\n",
      "27     btc            MLP  0.038875  0.052721 -11108.652255   75390.607488\n",
      "28     btc            CNN  0.019057  0.026735  -3064.721038   31004.594180\n",
      "29     btc            RNN  0.055006  0.078380 -32900.082651  116681.520485\n",
      "30     btc           PINN  0.052964  0.068836 -19682.713927  111821.121931\n",
      "31     btc        Stacked  0.000084  0.000137      0.931765      37.200094\n"
     ]
    }
   ],
   "source": [
    "# crypto_time_series_forecast.py\n",
    "# ---------------------------------------\n",
    "# Time-Series Forecasting of Crypto Returns\n",
    "# Models: PINN, CNN, RNN, MLP, BayesianRidge, XGBoost, AdaBoost, Stacked Ensemble\n",
    "# Uses TimeSeriesSplit for robust time-series cross-validation on one-minute data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.ensemble import AdaBoostRegressor, StackingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Sequential, callbacks\n",
    "\n",
    "# 1) LOAD DATA\n",
    "paths = {\n",
    "    'ada':   '/Users/jadenfix/downloads/processed_data_ada.csv',\n",
    "    'solana':'/Users/jadenfix/downloads/processed_data_solana.csv',\n",
    "    'eth':   '/Users/jadenfix/downloads/processed_data_eth.csv',\n",
    "    'btc':   '/Users/jadenfix/downloads/processed_data_btc.csv'\n",
    "}\n",
    "data = {coin: pd.read_csv(path, parse_dates=['timestamp']) for coin, path in paths.items()}\n",
    "\n",
    "# 2) UTIL FUNCTIONS\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute regression metrics including MAPE with safe handling of zero targets.\n",
    "    \"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    # Avoid division by zero in MAPE\n",
    "    mask = np.abs(y_true) > 1e-8\n",
    "    if np.any(mask):\n",
    "        mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "    else:\n",
    "        mape = np.nan\n",
    "    return {'mae': mae, 'rmse': rmse, 'r2': r2, 'mape': mape}\n",
    "\n",
    "# Generic TimeSeries CV for sklearn-style models\n",
    "\n",
    "def cv_sklearn_model(model, X, y, n_splits=3):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    fold_metrics = []\n",
    "    for train_idx, test_idx in tscv.split(X):\n",
    "        X_tr, X_te = X[train_idx], X[test_idx]\n",
    "        y_tr, y_te = y[train_idx], y[test_idx]\n",
    "        model.fit(X_tr, y_tr)\n",
    "        preds = model.predict(X_te)\n",
    "        fold_metrics.append(metrics(y_te, preds))\n",
    "    return {k: np.mean([fm[k] for fm in fold_metrics]) for k in fold_metrics[0]}\n",
    "\n",
    "# Prepare features/target\n",
    "def prepare_xy(df, coin):\n",
    "    df = df.sort_values('timestamp').dropna()\n",
    "    X = df.drop(columns=[f'{coin}_r_next', 'timestamp']).values\n",
    "    y = df[f'{coin}_r_next'].values\n",
    "    return X, y\n",
    "\n",
    "# 3) SKLEARN MODEL FUNCTIONS\n",
    "\n",
    "def bayesian_ridge_cv(X, y):\n",
    "    return cv_sklearn_model(BayesianRidge(), X, y)\n",
    "\n",
    "\n",
    "def xgboost_cv(X, y):\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=50,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        objective='reg:squarederror',\n",
    "        verbosity=0\n",
    "    )\n",
    "    return cv_sklearn_model(model, X, y)\n",
    "\n",
    "\n",
    "def adaboost_cv(X, y):\n",
    "    base = DecisionTreeRegressor(max_depth=3)\n",
    "    model = AdaBoostRegressor(\n",
    "        estimator=base,\n",
    "        n_estimators=30,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "    return cv_sklearn_model(model, X, y)\n",
    "\n",
    "# 4) NEURAL NET MODELS\n",
    "\n",
    "# MLP\n",
    "\n",
    "def build_mlp(n_features):\n",
    "    return Sequential([\n",
    "        layers.Input(shape=(n_features,)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "def mlp_cv(X, y, n_splits=3, epochs=10, batch_size=256):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    fold_metrics = []\n",
    "    for tr_idx, te_idx in tscv.split(X):\n",
    "        X_tr, X_te = X[tr_idx], X[te_idx]\n",
    "        y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "        scaler = StandardScaler().fit(X_tr)\n",
    "        X_tr_s = scaler.transform(X_tr)\n",
    "        X_te_s = scaler.transform(X_te)\n",
    "        model = build_mlp(X_tr_s.shape[1])\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        model.fit(\n",
    "            X_tr_s, y_tr,\n",
    "            validation_data=(X_te_s, y_te),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[callbacks.EarlyStopping(patience=2, restore_best_weights=True)],\n",
    "            verbose=0\n",
    "        )\n",
    "        preds = model.predict(X_te_s).flatten()\n",
    "        fold_metrics.append(metrics(y_te, preds))\n",
    "        tf.keras.backend.clear_session()\n",
    "    return {k: np.mean([fm[k] for fm in fold_metrics]) for k in fold_metrics[0]}\n",
    "\n",
    "# CNN\n",
    "\n",
    "def build_cnn(n_features):\n",
    "    return Sequential([\n",
    "        layers.Input(shape=(n_features, 1)),\n",
    "        layers.Conv1D(32, kernel_size=3, activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "def cnn_cv(X, y, n_splits=3, epochs=10, batch_size=256):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    fold_metrics = []\n",
    "    for tr_idx, te_idx in tscv.split(X):\n",
    "        X_tr, X_te = X[tr_idx], X[te_idx]\n",
    "        y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "        scaler = StandardScaler().fit(X_tr)\n",
    "        X_tr_s = scaler.transform(X_tr)[..., np.newaxis]\n",
    "        X_te_s = scaler.transform(X_te)[..., np.newaxis]\n",
    "        model = build_cnn(X_tr.shape[1])\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        model.fit(\n",
    "            X_tr_s, y_tr,\n",
    "            validation_data=(X_te_s, y_te),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[callbacks.EarlyStopping(patience=2, restore_best_weights=True)],\n",
    "            verbose=0\n",
    "        )\n",
    "        preds = model.predict(X_te_s).flatten()\n",
    "        fold_metrics.append(metrics(y_te, preds))\n",
    "        tf.keras.backend.clear_session()\n",
    "    return {k: np.mean([fm[k] for fm in fold_metrics]) for k in fold_metrics[0]}\n",
    "\n",
    "# RNN\n",
    "\n",
    "def build_rnn(n_features):\n",
    "    return Sequential([\n",
    "        layers.Input(shape=(1, n_features)),\n",
    "        layers.SimpleRNN(32, activation='tanh'),\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "def rnn_cv(X, y, n_splits=3, epochs=10, batch_size=256):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    fold_metrics = []\n",
    "    for tr_idx, te_idx in tscv.split(X):\n",
    "        X_tr, X_te = X[tr_idx], X[te_idx]\n",
    "        y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "        scaler = StandardScaler().fit(X_tr)\n",
    "        X_tr_s = scaler.transform(X_tr).reshape(-1, 1, X_tr.shape[1])\n",
    "        X_te_s = scaler.transform(X_te).reshape(-1, 1, X_te.shape[1])\n",
    "        model = build_rnn(X_tr.shape[1])\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        model.fit(\n",
    "            X_tr_s, y_tr,\n",
    "            validation_data=(X_te_s, y_te),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[callbacks.EarlyStopping(patience=2, restore_best_weights=True)],\n",
    "            verbose=0\n",
    "        )\n",
    "        preds = model.predict(X_te_s).flatten()\n",
    "        fold_metrics.append(metrics(y_te, preds))\n",
    "        tf.keras.backend.clear_session()\n",
    "    return {k: np.mean([fm[k] for fm in fold_metrics]) for k in fold_metrics[0]}\n",
    "\n",
    "# 5) PINN: Physics-Informed NN enforcing smoothness\n",
    "class PINN(Model):\n",
    "    def __init__(self, n_features, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.d1 = layers.Dense(64, activation='tanh')\n",
    "        self.d2 = layers.Dense(64, activation='tanh')\n",
    "        self.out = layers.Dense(1)\n",
    "        self.alpha = alpha\n",
    "    def call(self, x):\n",
    "        z = self.d1(x)\n",
    "        z = self.d2(z)\n",
    "        return self.out(z)\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)\n",
    "            data_loss = tf.reduce_mean(tf.square(y - y_pred))\n",
    "            with tf.GradientTape() as g2:\n",
    "                g2.watch(x)\n",
    "                y_hat = self(x)\n",
    "            dy_dx = g2.gradient(y_hat, x)\n",
    "            phys_loss = tf.reduce_mean(tf.square(dy_dx))\n",
    "            loss = data_loss + self.alpha * phys_loss\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        return {'loss': loss, 'data_loss': data_loss, 'phys_loss': phys_loss}\n",
    "\n",
    "def pinn_cv(X, y, n_splits=3, epochs=10, batch_size=256, alpha=1.0):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    fold_metrics = []\n",
    "    for tr_idx, te_idx in tscv.split(X):\n",
    "        X_tr, X_te = X[tr_idx], X[te_idx]\n",
    "        y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "        scaler = StandardScaler().fit(X_tr)\n",
    "        X_tr_s = scaler.transform(X_tr)\n",
    "        X_te_s = scaler.transform(X_te)\n",
    "        model = PINN(X_tr.shape[1], alpha)\n",
    "        model.compile(optimizer='adam')\n",
    "        model.fit(X_tr_s, y_tr, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "        preds = model.predict(X_te_s).flatten()\n",
    "        fold_metrics.append(metrics(y_te, preds))\n",
    "        tf.keras.backend.clear_session()\n",
    "    return {k: np.mean([fm[k] for fm in fold_metrics]) for k in fold_metrics[0]}\n",
    "\n",
    "# 6) Stacked Ensemble\n",
    "def stacked_cv(X, y, n_splits=3):\n",
    "    estimators = [\n",
    "        ('bayes', BayesianRidge()),\n",
    "        ('xgb', xgb.XGBRegressor(n_estimators=30, max_depth=3, learning_rate=0.1, objective='reg:squarederror', verbosity=0)),\n",
    "        ('ada', AdaBoostRegressor(estimator=DecisionTreeRegressor(max_depth=3), n_estimators=20, learning_rate=0.1, random_state=42))\n",
    "    ]\n",
    "    stack = StackingRegressor(estimators=estimators, final_estimator=BayesianRidge())\n",
    "    return cv_sklearn_model(stack, X, y)\n",
    "\n",
    "# 7) RUN ALL MODELS\n",
    "def run_all(data_dict):\n",
    "    records = []\n",
    "    for coin, df in data_dict.items():\n",
    "        X, y = prepare_xy(df, coin)\n",
    "        records.append({'coin': coin, 'model': 'BayesianRidge', **bayesian_ridge_cv(X, y)})\n",
    "        records.append({'coin': coin, 'model': 'XGBoost', **xgboost_cv(X, y)})\n",
    "        records.append({'coin': coin, 'model': 'AdaBoost', **adaboost_cv(X, y)})\n",
    "        records.append({'coin': coin, 'model': 'MLP', **mlp_cv(X, y)})\n",
    "        records.append({'coin': coin, 'model': 'CNN', **cnn_cv(X, y)})\n",
    "        records.append({'coin': coin, 'model': 'RNN', **rnn_cv(X, y)})\n",
    "        records.append({'coin': coin, 'model': 'PINN', **pinn_cv(X, y)})\n",
    "        records.append({'coin': coin, 'model': 'Stacked', **stacked_cv(X, y)})\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == '__main__':\n",
    "    results = run_all(data)\n",
    "    print(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
